\section{Computation}
The computational core for [[coxme]] iteration is contained in two
routines [[coxfit7b]] and [[agfit7b]], the former for the standard
case and the latter for (start,stop] data.
This routine will be called several hundred times during a fit, so 
the primary consideration is speed.
As a start, the input arguments are minimal, anything that persists
from call to call is dealt with by the setup and cleanup routines.
Arguments are:
\begin{description}
  \item[iter] A two element vector containing the minimum and maximum number
    of iterations.  For calls that are subservient to the [[optim]] function
    (almost all) these will set to force a fixed number of iterations.
  \item[beta] On input contains the starting values for iteration, on output
    contains the solution.
  \item[pmatb] The block diagonal portion of the penalty matrix (may be null)
  \item[pmatr] The dense portion of the penalty matrix.  Note that the for
    this routine the penalty matrix is a fixed quantity.
  \item[maxiter] The number of iterations actually done
  \item[loglik] The partial likelihood at the starting and ending value of beta
  \item[hdet] The log(determinant) of the sparse portion of H
\end{description}

The structure of the routine is below.  The line that defines ``c7'' is
simply so that the routine can use names like [[c7.n]] instead of the 
longer [[coxfit7.n]] for elements of the [[coxfit7]] structure.

<<coxfit7>>=
#include "coxmeS.h"
#include "coxfit6.h"
#include <math.h>
#include "bdsmatrix.h"
#define c7 coxfit7  
static void update(int j, int upper);
static double dsum1, dsum2;
static int nvar3;

SEXP coxfit7b(SEXP maxiter,  SEXP beta,
	      SEXP pmatb,    SEXP pmatr) {
    <<declare-variables-7b>>
    <<setup-data-7b>>
    <<iterate-7b>>
    <<finish-7b>>
    }
@     
    
You can usually skip reading the following section. 
<<declare-variables-7b>>=  
<<addmat>>   
int i,j,k, l, p;
int ii, istrat;
int    iter;
int    nvar, nvar2, nvar2b;
int    nfrail, ns, nfac;
int    nfns;    /* number of factors that are not sparse */
int    halving;

double  denom, zbeta, risk;
double  temp, temp2;
double  newlik;
double  d2, efron_wt;
double  ndead;
double  *dptr;
double  *psum;
int     dohalf =0;
int     ntie, ntie2, dup1, dup2;

nfrail = c7.nfrail;    /* number of penalized coefficients */
nvar   = c7.nvar;      /* number of unpenalized coefficients */
ns     = c7.nsparse;   /* dimension of the sparse part of penalty */
nimap  = c7.nimap;     /* number of columns of imap  */
nxmap  = c7.nxmap;     /* number of columns of xmap  */
nmap   = c7.nmap;      /* number of mapped coefficients  */
nvar2  = nvar + nxmap; /* number of cols of X */
nvar3  = nvar + nmap;  /* total number of coefficients */
for (i=0; i<nvar3; i++) c7.oldbeta[i] = (REAL(beta))[i];
@

Let $b$ be a set of intercept terms indexed by one of the columns of [[imat]].
That means that exactly one element of $b$ applies to each row of data, and
that the partial likelihood is identical for $b$ and $b +c$ where $c$
is a constant.  The penalty $(b+c)'\Sigma (b+c)$ is minimized when \ldots,
which implies the constraint that \ldots.
When the penalty matrix is full, the Newton-Raphson iteration keeps this
constraint in force automatically, but when it is sparse we can speed up
the iteration by adding the constraint as an extra step.  
Compute the sums of the penalty matrix and save the result in [[psum]].
<<setup-data-7b>>=
psum = c7.temp + nvar3;
for (i=0; i<c7.nfx; i++) {
    /* 
    ** get the product of the penalty matrix with the 0/1 indicator
    **  variable that represents the ith sparse factor variable.
    ** recentering the sparse factors is important for the NR; non-sparse
    **  terms don't have the problem.
    */
    bdsmatrix_prod2(c7.nblock, c7.bsize, nfrail, pmatb, pmatr,
			c7.findex + i*nfrail, c7.temp, c7.itemp); 
    temp =0;
    for (j=0; j<nfrail; j++) temp += c7.temp[j];
    psum[i] = temp;
    }
@

In a normal Cox model we start each iteration by setting the first derivative
vector ([[u]]) and
second derivative matrix ([[imat]]) to zero.  
Our first task in a penalized model, however, is to add
$-b P$ to the first [[nfrail]] elements of [[u]] and $P$ to the upper
left corner of [[imat]].  
Remember that the block and dense parts of [[imat]] are kept as separate
objects.
<<iterate-7b>>=
halving =0 ;             /* =1 when in the midst of "step halving" */
for (iter=0; iter<= maxiter[1]; iter++) {
    /*
    ** First the information matrix
    */
    for (i=0; i<c7.tblock; i++) 
	c7.imatb[i] = pmatb[i];
    dptr = pmatr;
    for (i=ns; i<nfrail; i++) {
	/* dense rows of penalty */
	for (j=0; j<nfrail; j++) c7.imat[i][j] = *dptr++;
	for (j=nfrail; j<nvar3; j++) c7.imat[i][j] =0;
	}
    for (i=nfrail; i<nvar3; i++) {
	/* unpenalized part */
	for (j=0; j<nvar3; j++) c7.imat[i][j] =0;
	}

    /* form the product of penalty times beta, save in c7.temp */
    bdsmatrix_prod2(c7.nblock, c7.bsize, nfrail, pmatb,
		    pmatr, beta, c7.temp, c7.itemp);
    
    /* u and penalized loglik */
    temp =0;
    for (i=0; i<nfrail; i++) {
	c7.u[i] = -c7.temp[i];
	temp += c7.temp[i]*beta[i];
	}
    newlik = -temp/2;  /* -(1/2) b' \sigma^{-1}b */
    for (i=nfrail; i<nvar3; i++) c7.u[i] =0;

    istrat=0;
    ntie =0;  ntie2=0;
    for (p=0; p<c7.n; p++) {  /* p = person */
        <<strata-reset-7b>>
	<<risk-score-7b>>
	<<addup-7b>>
	}
    }
@ 

The first task is to check if subject $p$ is the first of a new
strata.  If so, then the accumulation matrices for the running mean
and variance calculation need to be reset to zero.  
This is guarranteed to occur for the first subject in the data.
<<strata-reset-7b>>=
p==0 || p==c7.strata[istrat]) {
if (p>0) {
    istrat++;
    if (c7.calc2==1) {
	for (j=0; j<ns; j++) update(j, 0);
	}
    }

efron_wt =0;
denom = 0;
for (i=0; i<nvar3; i++) {
    c7.a[i] = 0;
    c7.a2[i]=0 ;
    for (j=0; j<nvar2b; j++) {
	c7.cmat[j][i] = 0;
	c7.cmat2[j][i]= 0;
        }
    }
if (c7.calc2 ==1) {
    dsum1 = 0; dsum2 =0;
    for (i=0; i<nvar2b; i++) c7.dsum3[i] =0;
    for (i=0; i<ns; i++) c7.dlag1[i] =0;
    for (i=0; i<c7.tblock; i++) c7.dlag2[0][i] =0; 
    for (i=ns; i<nvar3; i++) {
	for (j=0; j<=i; j++) c7.dlag2[i][j] =0;
	}
    }
}
@ 

Next we add up the linear predictor zbeta for each subject,
and compute their risk score.  
The first [[nmap]] coefficients in the $\beta$ vector correspond
to variables refenced through the [[imap]] and [[xmap]] matrices.
Each column of [[imap]] refers to a disjoint set of intercepts, i.e.,
each observation in the data uses one and only one of the intercept
terms mentioned in that column.  
They might be for instance coefficients 2, 7, 14, and 15 for imap
column 1; if observation $p$ uses coefficient 2 we don't need to
add $0\cdot\beta_7 + 0\cdot\beta_{14} +  0\cdot\beta_{15}$ into the risk score.

Similarly, the first column of [[xmap]] describes coefficient mappings
for the first column of the covariate matrix [[c7.x]].  This would
happen for instance with a set of per-institution treatment coefficients:
[[x]] would contain the treatment variable for all subjects, but there
is a separate $\beta$ coefficient for each institution.  
Last, we add in the ``ordinary'' X variables, which start at column
[[nxmap]] of the X matrix and coefficient position [[nmap]].
Note that [[nimap]] and [[nxmap]] are the number of columns of the
[[imap]] and [[xmap]] matrices, and [[nmap]] is the total number of coefficients
that these matrices map into.
<<risk-score-7b>>=
zbeta = c7.offset[p];
for (i=0; i<nimap; i++) {
    j = c7.imap[p + i*c7.n];  /* level of covariate i */
    zbeta += beta[j];
    }
for (i=0; i<nxmap; i++) {
    j = c7.xmap[p + i*c7.n];
    zbeta += beta[j]* c7.x[i][p];
    }
for (i=0; i<nvar2; i++)
    zbeta += beta[i+nmap]* c7.x[i+ nxmap][p];
risk = exp(zbeta) * c7.weights[p];
denom += risk;
@

Now to work --- but with considerable care if we want the routine to be
fast.  The two primary quantities to be calculated are the weighted mean and
variance at each unique death time:
\begin{eqnarray}
  \xbar_j(t) &=& \frac{\sum_{i=1}^n Y_i(t) r_i x_{ij}}
                      {\sum_{i=1}^n Y_i(t) r_i} \label{eq:xbar}\\
  {\rm V}_{jk}(t)&=& \frac{\sum_{i=1}^n Y_i(t) r(t) (x_{ij}- \xbar_j(t))
                                                      (x_{ik}- \xbar_k(t))}
                      {\sum_{i=1}^n Y_i(t) r_i} \label{eq:var1}\\ 
               &=& \frac{\sum_{i=1}^n Y_i(t) r_i (x_{ij} x_{ik})}
                       {\sum_{i=1}^n Y_i(t) r_i} - \xbar_j(t)\xbar_k(t)
                       \label{eq:var2}
\end{eqnarray*} 
The data is arranged so that we proceed from the longest time to the
shortest so that these terms accumulate. 
The variables defined just above are [[risk]]=$r_i$ and [[denom]] = the
running sum of $r_i$ = the denominator of equations 
\ref{eq:xbar} and \ref{eq:var1}.
The numerator of equation \ref{eq:xbar} is kept in the vector [[a]] and
the numerator of equation \ref{eq:var2} is kept in [[cmat]].
For the Efron approximation we will also need these sums within the
set of subjects who are tied, these variables are called [[efron_wt]] (for
the denominator), [[a2]] and [[cmat2]].
At each death the primary variables of the computation, namely the
score vector [[u]] and the information matrix [[imat]], are updated 
using these quantities.

Exactly \emph{how} to keep these quantities updated is the challenge.
Start with a particular motivating case.  In the Minnesota Breast Cancer
Family Study we fit a large number of models on $n=12701$ females
from 426 families, with
a random effect per subject and a small number of covariates $p$; $p$ was
normally 5 or less.  There were $d=637$ incident breast cancers observed
during the followup.

The single random effect is referenced as a single column of [[imap]],
which has 12701 rows, each a unique value.  A given subject with index
$k$ affects only one of the first 12701 elements of the $a$ vector (which
is of length $12701 +p$, namely $a_k$.  All $p$ elements for the
ordinary covariates must be updated, leading to an overall effort
of $O(n + np)$. Compared to an ordinary Cox model with one column of
$X$ for each of the intercepts and consequent computation of
$O(n(n+p))$ this is a huge savings.
Likewise, only $C_{kk}$ is
impacted in the $12710\times 12701$ upper left corner of [[cmat]]; all other
products for the coefficents relating to this column of [[imap]]
will involve a zero.  
(Since $x^2=x$ for a 0/1 intercept variable we need not even keep these
elements of $C$ since they are equal to $a$.  However, since intercepts and
slopes might be intermixed in the coefficient vector, it is simpler to
keep the redundant value).  There are also $kp$ cross-products between this
factor and the covariates, and the $p \times p$ dense lower right corner
for the covariates, leading to $O(n(1+p + p^2))$.
A similar argument holds for [[xmap]]. 

We also need to deal with sparsity.  The penalty matrix as input to
[[coxfit7a]] determines sparsity for [[cmat]] and [[imat]] as well.
Sparse terms are restricted, however, to the mapped coefficients.
The [[addmat]] routine determines if a coefficient is present, and if
so adds the argument to the relevant matrix.
At present the standard variance routines [[coxmeFull]] and [[coxmeMlist]]
only use sparse terms for the first mapped variable, but we allow
more generality for user written routines.

Below is the code.  
Ignore for a moment the calls to [[update]].
<<compute-c7>>= 
for (i=0; i< nimap; i++) {
    j = c7.imap[p + i*c7.n];   /* jth covariate is = to 1 */
    /* first, update u and imat based on the OLD a[j] */
    update(j,1);
    
    /* Now update a and cmat */
    c7.a[j] += risk;
    c7.cmat[j][j] += risk;
    for (k=i+1; k<nimap; k++) /* cross products with other intercepts */
	addmat(c7.cmat, j, c7.imap[p+ k*c7.n], risk);
    for (k=1; k<nxmap; k++)
	addmat(c7.cmat, j, c7.xmap[p +k*c7.n], risk* c7.x[k][p]);
    for (k=0; k<nvar2; k++) /* covariates */ 
	c7.cmat[k+nmap][j] += risk*c7.x[k+nxmap][p];
    }

for (i=0; i<nxmap; i++) {
    j=c7.xmap[p + i*c7.n];
    c7.a[j] += risk* x[i][p];
    c7.cmat[j][j] += risk* x[i][p]* c7.x[i][p];
    for (k=i+1; k<nxmap; k++)
	addmat(c7.cmat, j, c7.xmap[p+ k*c7.n], risk* c7.x[i][p]* c7.x[k][p]);
    for (k=0; k<nvar2; k++)
	c7.cmat[k+nmap][j] += risk *c7.x[k+nxmap][p] * c7.x[i][p];
    }
	    
for (i=0; i<nvar2; i++) {   /* non-factor variables */
    for (j=0; j<=i; j++)
	c7.cmat[i+nmap][j+nmap] += risk* c7.x[i+nxmap][p]* c7.x[j+nxmap][p];
    }
@ 

If the subject is a death we need to add the current covariate to the score
statistic $u$.  For the Efron approximation, we aso 
repeat the above code to add to  [[a2]] and [[cmat2]].  
These are the portions of [[a]] and [[cmat]] that have been accumulated
at this particular death time, and are needed whenever there are tied
death times.
<<compute-c7>>=
if (c7.status[p]==1) {
    newlik += c7.weights[p] *zbeta;
    efron_wt += risk;
    for (i=0; i<nimap; i++) {
	j = c7.imap[p + i*c7.n];   /* jth covariate is = to 1 */
	c7.u[j] += c7.weights[p];
	if (c7.method==1) { /*only needed for the Efron approx */
	    c7.a2[j] += risk;	
	    c7.cmat2[j][j] += risk;
	    for (k=i+1; k<nimap; k++)
		addmat(c7.cmat2, j, c7.imap[p+ k*c7.n], risk);
	    for (k=0; k<nxmap; k++)
		addmat(c7.cmat2, j, c7.xmap[p +k*c7.n], risk* c7.x[k][p]);
	    for (k=0; k<nvar2; k++)
		addmat(c7.cmat2, j, k+nmap, risk* c7.x[k+nxmap][p]);
	    }
	}
    for (i=0; i<nxmap; i++) {
	j = c7.xmap[p + i*c7.n];
	c7.u[j] += c7.weights[p]*c7.x[i][p];
	if (c7.method==1) {
	    temp = c7.x[i][p];
	    c7.a2[j] += risk* temp;
	    for (k=i+1; k<nxmpap; k++)
		addmat(c7.cmat2, j, c7.xmap[p +k*c7.n], risk* c7.x[k][p] *temp);
	    for (k=0; k<nvar2; k++)
		addmat(c7.cmat2, j, k+nmap, risk*c7.x[k+nxmap][p] *temp);
	    }
	}
    for (i=0; i<nvar2; i++) {
	c7.u[i+nmap] += c7.weights[p] * c7.x[i][p];
	if (c7.method==1) {
	    temp = c7.x[i+nxmap][p];
	    c7.a2[i+nmap] += risk*temp;
	    for (k=i; i<nvar2; k++)
		c7.cmat2[k+nmap][i+nmap] += risk* temp * c7.x[k+nxmap][p];
	    }
	}
    }
@ 

Here is the definition of addmat.  
We make it a macro rather than a function for speed.  In particular,
the multiplication often found in the last term won't be done
if the element is omitted from the sparse matrix.
The parent routine can guarrantee semi-ordering of the coefficients:
if both $i$ and $j$ are in the sparse portion then $j>i$, and 
sparse before non-sparse of course. 
We would get $i>j$ for a model with random terms of
\verb=(1+trt|grp1) + (1|grp2)= where the first grouping is sparse
for both treatment and intercept and the second group in non-sparse,

Almost always, we expect that $j> ns$, so deal with that case first.
(Normally sparseness can only be asserted for a single grouping variable.)
It is an artifact of bdsmatrix objects that the sparse portion appears
to be a row-major order and the dense portion in column major order.
The 
<<addmat>>=
#define addmat(matrix, i, j, x)
if (i>j)    matrix[i][j] += x;   /* only possible if i>= ns */
else if (j >=ns) matrix[j][i] += x;   /* in the dense part */
else if (j <= dense_max[i]) matrix[i][j] += x;
#end
@ 

A second improvment in speed comes from when adding up the
score statistic for a mapped coefficient..
At each death time the mean covariate $\xbar_k(t) =a_k(t)/d(t)$ is subtracted
from element $k$ of the score vector $u$.  
For the Minnesota study this is $635 \times 12701$ additions and divisions,
which takes up far more time than all of the work so far. This can
be considerably reduced by noting that any given element of $a$
changes only once: when subject $k$ enters the risk set (remember we
iterate backwards in time).  If we keep a running sum of the 
denominators $d_{tot}= 1/d(t_n) + 1/d(t_{n-1} + \ldots 1/d(t_1)$ it is only
necessary to remember what the total was just before each person
started and do a single multiplication $a_k(t_1)(d_{tot} - d_k)$.
This reduces the compute time by a factor of 631 for a modest amount
of extra bookkeeping.

More generally, for any covariate we have the running sum of $a$
along with the accumulated denominator terms.
\begin{centering}
  \begin{tabular}{r|rrrrrrrrrrrr} 
    &\multicolumn{12}{c}{Time points} \\ \hline
 Sparse covariate & 0&0&0&0&0&0&1&0&0&0&0& \ldots \\
 Dense covariate &  1&5&2&7&8&0&3&2&6&8&5& \ldots \\
 Death times     &  &&&d_1&&&&&d_2&&&&&\ldots \\
 \end{tabular}
\end{centering}
For each covariate $a$ is a running sum.  We need to add to the
the score statistic whenever the sum has changed \emph{and} one or
more deaths have occurred. For very sparse covariates the first is
the rarer event, but for most covariates death is the rarer one.
A reasonable cutpoint is if the number of coefficients for a mapped
variable is greater than the number of deaths.  
We don't have to be perfect as long as we choose the right direction
for the Minnesota Breast study on one hand, and for a study with a
nearly dense covariate, 9000 subjects and 100 deaths on the other.

Computation for the cross-products matrix is a little more complex.
The basic trick is the identiy for a covariance
$$
   (1/n)\sum (x-\xbar)(y-\overline y) = (1/n)\sum x (y-\overline y)
$$
true for any pair of variables $x$ and $y$.  
The Cox information is a sum of such terms, one per death time.
Let $x$ be a sparse covariate by the above definition.
We update the cross product as we would for the sparse term,
and treat the running sum of $a/d$ for the second covariate as
we did the $1/d$ sums.  

In summary [[cmat]] retains the running sums of cross products or
deferred sums, depending on the term.  
The remaining questions are what to do with two mapped terms, and
interactions with a sparse penalty matrix.
\begin{itemize}
  \item If only one of the mapped terms has a large enough set of levels
    to demand sparse computation, the solution is to treat all the others
    in the same way as the dense variables.
  \item The most common situation with multiple sparse will be a shared
      grouping variable (the case of \verb=(1+trt|inst)= for instance.
      The update strategy is then of the same order of work: sets of 
      elements of $a$ change together and the total number of updates remains
      small (only a few variables have changed at any death).
  \item The situation of two crossed terms, each with a very large number of
    levels, is not expected to arise very often.  (Compute times may be
    very very long).  Because cross terms will need to update whenever
    either grouping variable is updated, the deferred computation is unlikely
    to genererate benefit, so we choose one of them to be ``sparse''.
    We want to choose the one with the larger number of levels,
    and this will almost always have been placed first in the model to
    take advantage of a sparse penalty (only the first penalty is
    allowed to be block diagonal). We thus key on the first.
\end{itemize}

For the Efron approximation we have similar issues, but only for the 
set of subjects who had a tied death at some time. Since this
is likely to be small, we keep a vector list of ``possibly affected
coefficients''.  

<<compute-7b>>=
if (status[p]==1 && c7.method==1) {
    for (i=0; i<nimap; i++) {
	j = c7.imap[p + i*c7.n];
	dup1=0;
	for (k=0; i<ntie; k++)
	    if (c7.tlist[k]==j) {
		dup1=1;
		break;
		}
	if (dup1==0) c7.tlist[ntie++] = j;
	}
    
    for (i=0; i<nxmap; i++) {
	j = c7.xmap[p * i*c7.n];
	dup1=0;
	for (k=0; i<ntie; k++)
	    if (c7.tlist[k]==j) {
		dup1=1;
		break;
		}
	if (dup1==0) c7.tlist[ntie++] = j;
	}
    }
@ 

Now the compuations that are done once per unique death time.
One nuisance of Cox models is that we have to defer calculation of
a mean when marching through time; because of tied times we don't  %'
know the true mean at that time until all the subjects at that time
have been added up.
<<compute-7b>>=
if (c7.mark[p] >0) {  /* once per unique death time */
  /* use cmat, cmat2, a, and a2 to update u and imat */
    ndead = c7.mark[p];
    if (c7.method==0 || ndead==1)  {
	/*
	** Breslow approx -- we can ignore a2 and cmat2
	*/
	temp = c7.wtave[p] * ndead;
	newlik -= temp *log(denom);

	ii = ns;
	dsum1 += temp/denom;
	dsum2 += temp/(denom * denom);
	
	for (i=ns; i<nvar3; i++) {  /* update u */
	    c7.temp[i] = c7.a[i]/ denom;
	    c7.u[i] -= temp *c7.temp[i];
	    c7.dsum3[i-ns] += c7.temp[i] * temp/denom;
	    }

	/* non-sparse variables - factor or continuous*/
	for (i=0; i<nvar2b; i++) {
	    k = i+ns;  /* i=row number in cmat, k= row in imat */
	    for (j=ii; j<=k; j++) 
		c7.imat[k][j] +=  temp *(
		    c7.cmat[i][j] /denom - c7.temp[k]*c7.temp[j]);
	    }
	}
		
    else {
	/* 
	** Do the Efron approx 
	** In this case we update the non-sparse, along with
	**  those sparse factors which got changed at this death
	**  time (those with a2 != 0)
	*/
	for (temp2=0; temp2<ndead; temp2++) {
	    temp = temp2/ ndead;
	    d2= denom - temp*efron_wt;
	    newlik -= c7.wtave[p] *log(d2);
	    
	    if (c7.calc2==1) {
		ii = ns;
		dsum1 += c7.wtave[p]/d2;
		dsum2 += c7.wtave[p]/(d2*d2);
		
		for (i=ns; i<nvar3; i++) {  /* update u */
		    c7.temp[i] = (c7.a[i] - temp*c7.a2[i])/d2;
		    c7.u[i] -= c7.wtave[p] *c7.temp[i];
		    c7.dsum3[i-ns] += c7.temp[i] * c7.wtave[p]/d2;
		    }

		for (i=0; i<ntie2; i++) {
		    for (j=c7.bstart[c7.tlist[i]]; 
			 j<c7.bstop[c7.tlist[i]]; j++) {
			c7.temp[j] = (c7.a[j] - temp*c7.a2[j])/d2;
			}
		    }
		for (i=0; i<ntie; i++) {
		    j = c7.tlist[i];
		    c7.u[j] -= c7.wtave[p] *c7.temp[j];
		    c7.imat[j][j] +=  c7.wtave[p] *c7.temp[j];
		    /*
		    ** Update imat[k,j] for all k, unless k<j 
		    **  and k is also on tlist (no double updates!)
		    */
		    for (k=c7.bstart[j]; k<j; k++) {
			dup1=0;
			for (l=0; l<ntie; l++)
			    if (c7.tlist[l]==k) dup1=1;
			if (dup1==0) 
			    c7.imat[k][j] -= c7.temp[j]*c7.temp[k]
				* c7.wtave[p];
			}
		    for (k=j; k<c7.bstop[j]; k++) 
			c7.imat[j][k] -= c7.temp[j]*c7.temp[k]
			    * c7.wtave[p];
		    for (k=ns; k<nvar3; k++) 
			c7.imat[k][j] += c7.wtave[p]* (
			    (c7.cmat[k-ns][j] - 
			     temp*c7.cmat2[k-ns][j])/d2 -
			    c7.temp[k]*c7.temp[j]);
		    }
		}
	    else {
		ii=0;
		for (i=0; i<nvar3; i++) {
		    c7.temp[i] = (c7.a[i] - temp*c7.a2[i])/d2;
		    c7.u[i] -= c7.wtave[p] *c7.temp[i];
		    }
		for (i=0; i<ns; i++) {	
		    c7.imat[i][i] += c7.wtave[p] *c7.temp[i];
		    for (j=i; j< c7.bstop[i]; j++)
			c7.imat[i][j] -= c7.wtave[p] *
			    c7.temp[i] * c7.temp[j];
		    }
		}

	    /*
	    ** Update the non-sparse part of imat
	    */
	    for (i=0; i<nvar2b; i++) {
		k = i+ns;  
		for (j=ii; j<=k; j++) {
		    c7.imat[k][j] +=  c7.wtave[p]*(
			(c7.cmat[i][j] - temp*c7.cmat2[i][j]) /d2 -
			c7.temp[k]*c7.temp[j]);
		    }
		}
	    }
		    
	if (c7.calc2 == 1) { /* update denominators */
	    for (i=0; i<ntie; i++) {
		j = c7.tlist[i];
		c7.dlag1[j] = dsum1;
		for (k=c7.bstart[j]; k<j; k++)
		    c7.dlag2[k][j] = dsum2;
		for (k=j; k<c7.bstop[j]; k++)
		    c7.dlag2[j][k] = dsum2;
		for (k=ns; k <nvar3; k++)
		    c7.dlag2[k][j] = c7.dsum3[k-ns];
		}
	    }
	} /* end of Efron loop */
			 
    /* rezero temps */
    efron_wt =0;
    ntie =0; ntie2=0;
    for (i=0; i<nvar3; i++) {
	c7.a2[i]=0;
	for (j=0; j<nvar2b; j++)  c7.cmat2[j][i]=0;
	}
    }   /* matches "if (mark[p] >0)"  */
} /* end  of accumulation loop  */
@ 


    /* 
    ** Finish up any deferred sums for sparse terms
    */
    if (c7.calc2==1) {
	for (j=0; j<ns; j++) update(j, 0);
	}
	    
    /* 
    **   Am I done?
    ** Note, when doing "minimum" iterations, don't allow step halving at
    **  the tail end of the iterations.  
    */
    if (iter==0) loglik[0] = newlik;
    if (iter>0 && newlik < loglik[1] && 
	fabs(1-(loglik[1]/newlik)) > c7.eps)  {  
	/*it is not converging ! */
	halving =1;
	dohalf = iter;
	for (i=0; i<nvar3; i++)
	    beta[i] = (c7.oldbeta[i] + beta[i]) /2; 
	continue;
	}

    halving =0;
    cholesky4(&(c7.imat[ns]), nvar3, c7.nblock, 
	      c7.bsize,  c7.imatb, c7.tolerch);

    if (iter >= maxiter[0] && fabs(1-(loglik[1]/newlik)) <= c7.eps) break;
    loglik[1] = newlik;
    if (iter < maxiter[1]) {
	chsolve4(&(c7.imat[ns]), nvar3, c7.nblock, 
		 c7.bsize,  c7.imatb, c7.u, 0);
	for (i=0; i<nvar3; i++) {
	    c7.oldbeta[i] = beta[i];
	    beta[i] += c7.u[i];
	    }
	    
	/*
	** Impose the constraint, mean frailty for any factor term
	**  is 0.  If the problem is not sparse, this happens
	**  automatically with the NR iteration.  If it is sparse,
	**  this helps efficiency of the maximizer.
	** c7.a is used as a scratch variable.  Each call to prod2
	**   is penalty matrix[rows/cols for this factor] %*% beta[this
	**   factor].  The divisor is penalty[same] %*% rep(1, nrows)
	*/
	for (i=0; i<c7.nfx; i++) {
	    for (j=0; j<nfrail; j++)
		c7.a[j] = beta[j] * c7.findex[j + i*nfrail]; 
	    bdsmatrix_prod2(c7.nblock, c7.bsize, nfrail, pmatb, pmatr,
			    c7.a, c7.temp, c7.itemp);
	    temp =0;
	    for (j=0; j<nfrail; j++) temp += c7.temp[j];
	    temp /= psum[i];  /* the mean */
	    for (j=0; j<nfrail; j++) {
		if (c7.findex[j + i*nfrail] ==1) beta[j] -= temp;
		}
	    }
	}
    }   /* return for another iteration */

temp =0;
for (i=0; i<nfrail; i++) temp += log(c7.imat[i][i]);
*hdet = temp;
if (maxiter[1] > iter) maxiter[1] = iter;
loglik[1] = newlik;
return;
}
@
static void update(int j, int upper) {
    double temp;
    int k;

    if (dsum1 == c7.dlag1[j]) return;  /* all the terms below just add a zero*/

    if (c7.a[j] > 0) {  /* for 1 factor/obs, this saves half the evals! */
	temp = c7.a[j] * (dsum1 - c7.dlag1[j]);
	c7.u[j] -= temp;
	c7.imat[j][j] += temp; 

	if (upper==1) {
	    for (k=c7.bstart[j]; k<j; k++) 
		c7.imat[k][j] -= c7.a[j]*c7.a[k] * (dsum2 - c7.dlag2[k][j]);
	    }

	for (k=j; k<c7.bstop[j]; k++) 
	    c7.imat[j][k] -= c7.a[j]*c7.a[k] * (dsum2 - c7.dlag2[j][k]);
	for (k=c7.nsparse; k<nvar3; k++) 
	    c7.imat[k][j] +=  c7.cmat[k-c7.nsparse][j]*(dsum1 - c7.dlag1[j]) -
	                   c7.a[j] *(c7.dsum3[k-c7.nsparse] - c7.dlag2[k][j]);
	}

    c7.dlag1[j] = dsum1;
    if (upper==1) for (k=c7.bstart[j]; k<j; k++) c7.dlag2[k][j] = dsum2;
    for (k=j; k<c7.bstop[j]; k++) c7.dlag2[j][k] = dsum2;
    for (k=c7.nsparse; k<nvar3; k++)   c7.dlag2[k][j]=c7.dsum3[k-c7.nsparse];
    }
