\documentclass{article}
\usepackage{noweb}
\usepackage[pdftex]{graphicx}
%\usepackage{times}
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{\oddsidemargin}

\newcommand{\myfig}[1]{\resizebox{\textwidth}{!}
                        {\includegraphics{figure/#1.pdf}}}

\noweboptions{breakcode}
\title{The \emph{coxme} function in S}
\author{Terry Therneau}

\begin{document}
\maketitle

\section{Introduction}
 The [[coxme]] function for mixed effects analyses based on a Cox
proportional hazards model is one of the later additions to the
[[survival]] library in S.
\footnote{S is a statistical language, S-Plus and R are two different
dialects of that language.  The functions described here work in both.}
It is easily the most complex bit of code in the package from all points
of view: mathematical, algorithmic, and S code wise.
As such, it seems like a natural candidate for documentation and 
maintainance using the literal programming model.  
\begin{quotation}
Let us change or traditional attitude to the construction of programs.
Instead of imagining that our main task is to instruct a \emph{computer}
what to do, let us concentrate rather on explaining to \emph{humans}
what we want the computer to do.  (Donald E. Knuth, 1984).
\end{quotation}

This document is my first foray into said domain.  
Time will tell if it is successful in creating both more reliable and
better understood code.

First, note that almost all of the code uses a \emph{.Rnw} suffix.
However, it is not processed by the [[Sweave]]
package, which is designed for reports containing 
\emph{exectuted} S code.   
We don't want this material run through the [[Sweave]]
command, because it replaces our carefully formatted S code
with the far less readable results echoed by the R parser.
Because it can't tell where R output
will erupt, [[Sweave]] is forced to echo in this fashion. 
Since this document is intended to both create the [[coxme]] code
sources along with a technical docment describing them, we use the
standard noweb interface to process them, with a Makefile to drive
the process.

\section{Main program}
The [[coxme]] code starts with a fairly standard argument list.
<<coxme>>=
coxme <- function(formula,  data, 
	weights, subset, na.action, init, 
	control, ties= c("efron", "breslow", "exact"),
	singular.ok =T, varlist=NULL, variance, vinit=.2, sparse=c(50,.02),
	rescale=TRUE, pdcheck=TRUE, x=FALSE, y=TRUE, shortlabel=TRUE, 
        refine.n=0, random, fixed, ...) {

    time0 <- proc.time()    #debugging line
    ties <- match.arg(ties)
    Call <- match.call()

    <<process-standard-arguments>>
    <<decompose-formula>>
    <<build-control-structures>>
    <<call-computation-routine>>
    <<finish-up>>
@ %def coxme

The arguments to the function are described below,
omitting those that are identical to the [[coxph]] function.
\begin{description}
\item[formula] The formula desribing the fixed and random effects.  This
will be discussed in detail below.
\item[varlist] An optional list, with one element per random term, that
describes the variance structure of the random effects.
\item[variance] An optional list (vector) of fixed values for selected variance
components.  
\item[vinit] Initial value(s) for the variance components in the iteration.
\item[sparse] The rule for deciding that the estimation of a term should be
sparse.  
\item[shortlabel] An option that applies to the creation of coefficient
labels for nested effects.  See the [[strata]] function for more
information.
\item[refine.n] The number of Monte Carlo iterations to be done at the final
iteration, to refine the Laplace approximation of the likelihood.
\item[random, fixed] These are included for backwards compatability 
with the first verion of coxme.  They may be removed at some
point in the future.
\end{description}

The [[sparse]] option requires some further discussion.
Cox model variance matrices are never sparse, but we have found that
in one very particular instance we can ignore many off-diagonal
5elements.  This combined with the naturally sparse structure of
the penalty matrix can lead to substantial reductions in
computational time.  
The ignorable elements arise for a random intercept term.  In this case
the diagonal elements of the usual Cox model variance matrix
are $O(p_i)$ and the off diagonals are $O(p_i p_j)$, where $p_i$ is
the fraction of subjects in group $i$.
If both $p_i$ and $p_j$ are sufficiently small the corresponding
off-diagonal may be effectively ignored.
For a particular family study that motivated the code there were
over twenty thousand subjects, with a random intercept per subject, and
the computation was not feasable without this addition.
Based on fairly limited experience, the lower level for the
approximation is set at $p= 1/50$.  
The default values for the [[sparse]] option state that the approximation
should only be used if there are $>50$ levels for the grouping factor, and
only for those groups representing .02 or less of the total.
If there are multiple random effects only one is allowed a sparse
representation, nor are random slopes ever represented in this way.
Further reseach may reveal wider circumstances in which the approximation
is workable, but for now only the one known case is allowed.



\subsection{Basic setup}
The [[coxme]] code start its model handling with a nod to backwards
compatability.
The argument list starts with the usual [[(formula, data, weights, ...]]
set, but also allows [[random]] and [[fixed]] as optional arguments.
If they are present, it assumes that someone is using the old style,
and glues the fixed and random parts together into a single formula.
Because the old form had [[fixed]] as its first argument, we also
need to allow for the case where the user has assumed that the first,
unnamed argument to the call, which now maps to the [[formula]]
argument, is the fixed portion.
<<process-standard-arguments>>=
if (!missing(fixed)) {
    if (missing(formula)) {
        formula <- fixed
        warning("The 'fixed' argument of coxme is depreciated")
        }
    else stop("Both a fixed and a formula argument are present")
    }
if (!missing(random)) {
    warning("The random argument of coxme is depreciated")
    if (class(random) != 'formula' || length(random) !=2) 
        stop("Invalid random formula")
    j <- length(formula)   #will be 2 or 3, depending on if there is a y
    # Add parens to the random formula
    rtemp <- formula(paste('(', paste(deparse(random[[2]]), collapse=''), 
                                    ')'))  
    formula[[j]] <- call('+', formula[[j]], rtemp)  # paste it on
    }
@
A formula in S is represented as a list of length 2 or 3, whose first element
is as.name('~'), then the left hand side, if present, then the right hand side.
%Note that if the random statement is exceptionally long, then the [[deparse]]
%command will produce a vector of charcter strings, one for each line of 
%output if this were echoed to the terminal;
%the [[paste(... ,collapse)]] construction rejoins these.
Note that the old version of coxme contains almost the same code, since
to correctly handle missing values it needed to retrieve all the
relevant variables, both fixed and random, with a single list.

The program then executes a fairly standard step to retrieve the model
frame.  
The model.frame function does not correctly handle vertical bars in
a random term, the subbar function replaces each of these with a '+'.
<<process-standard-arguments>>= 
temp <- call('model.frame', formula= subbar(formula))
for (i in c('data', 'subset', 'weights', 'na.action'))
    if (!is.null(Call[[i]])) temp[[i]] <- Call[[i]]
if (is.R()) m <- eval.parent(temp)
else        m <- eval(temp, sys.parent())
@ 
The final line  is one of the few in the code that is specific to the
particular S engine being used.

One question that comes up when first seeing this code, is ``why not the
simpler code''
\begin{verbatim}
    temp <- model.frame(formula, data=data, subset=subset, 
                        weights=weights, na.action=na.action)
\end{verbatim}
The answer is that if any of the optional arguments were missing, then we would
get an error.  
What the code above does is to create the above call bit by bit. 
The starting point only includes the [[formula]] argument, which is
required.
Then any optional arguments that are actually present are copied over
from [[Call]] (what the user typed) to the temp variable.
Many older S functions take a different approach by the way.  They first made
a complete copy of the call, e.g. [[temp <- Call]], and then 
remove arguments that they don't want
[[temp$ties <- NULL; temp$rescale <- NULL]] etc.
I don't like this approach, since every time that a new argument
is added to the function, we need to remember to also add
it to this x-out list.  
Another alternate, found in much of the newer R code is
\begin{verbatim}
    alist <- match(names(Call), c('formula','data', 'subset', 'weights',
                                  'na.action')
    temp <- Call[c(1, which(!is.na(alist)))]
    temp[[1]] <- as.name('model.frame')
\end{verbatim}
My code above automatically forces an error if the formula is missing.

The model frame that we have created will contain all the variables found
in both the fixed and random portions of the model.
The next step is a usual one --- pull out special terms such as the response,
offset, etc.
Penalized terms are supported in [[coxph]] but are not allowed in [[coxme]]
The most common penalized terms in [[coxph]] are frailty terms
terms and psplines
(smoothing splines, similar to generalized additive models).
Frailty terms are simple shared random effects, 
it was an early way to get some of
the functionality of [[coxme]] by grafting a new capability onto [[coxph]].
Pspline terms could be supported, in theory, in [[coxme]], but the
effort to do so appears daunting and it is left for some future coder.

<<process-standard-arguments>>=
    Y <- model.extract(m, "response")
    n <- nrow(Y)
    if (!inherits(Y, "Surv")) stop("Response must be a survival object")
    type <- attr(Y, "type")
    if (type!='right' && type!='counting')
	stop(paste("Cox model doesn't support '", type,
			  "' survival data", sep=''))

    weights <- model.weights(m)
    if (length(weights) ==0) weights <- rep(1.0, n)
    else if (any(weights <=0))
        stop("Negative or zero weights are not allowed")

    offset <- model.offset(m)
    if (length(offset)==0) offset <- rep(0., n)

    # Check for penalized terms; the most likely is pspline
    pterms <- sapply(m, inherits, 'coxph.penalty')
    if (any(pterms)) {
	stop("You cannot have penalized terms in coxme")
	}
    if (missing(variance)) theta <- NULL
    else  theta <- variance 
@ 
The last two statements might be opaque: the technical documentation
for the mixed effects Cox model refers to the set of
variances as a vector $\theta$, and that is the notation used
from this point forward in the code.
But for users of the routine [[variance]] was felt to be a 
better label for the option.
%
% Second part of the main code
%
\subsection{Fixed effects}
The mixed effects Cox model is written as
\begin{eqnarray*}
  \lambda(t) &=& \lambda_0(t) e^{X \beta + Z b}\\
  b &\sim& N(0, \Sigma(\theta))
\end{eqnarray*}
The coefficient vectors $\beta$ and $b$ correspond the the
fixed and random effects, respectively,
with $X$ and $Z$ as the respective design matrices.

It is now time to build $X$, the design matrix for the fixed effects.
The
of the model. 
We first separate the model into random and fixed effects terms
using the [[formula1]] function.
As an argument it takes the model formula as given by the user
and it returns a list containing the fixed and random parts of
the formula, respectively.
If any vertical bars remain in the fixed result, then there is
a problem with the supplied formula, 
usually a random effects term that was missing the enclosing
parentheses.
<<decompose-formula>>=
    flist <- formula1(formula)
    if (hasAbar(flist$fixed))
        stop("Invalid formula: a '|' outside of a valid random effects term")

    special <- c("strata", "cluster")
    Terms <- terms(flist$fixed, special)
    attr(Terms,"intercept")<- 1  #Cox model always has \Lambda_0
    strats <- attr(Terms, "specials")$strata
    cluster<- attr(Terms, "specials")$cluster
    if (length(cluster)) {
        stop ("A cluster() statement is invalid in coxme")
        }
    if (length(strats)) {
        temp <- untangle.specials(Terms, 'strata', 1)
        dropx <- c(dropx, temp$terms)
        if (length(temp$vars)==1) strata.keep <- m[[temp$vars]]
        else strata.keep <- strata(m[,temp$vars], shortlabel=T)
        strats <- as.numeric(strata.keep)
        X <- model.matrix(Terms[-temp$terms], m)[,-1,drop=F]
        }
    else X <- model.matrix(Terms, m)[,-1,drop=F]


@ 
The key tools for building the matrix are the [[terms]] and [[model.matrix]]
functions, which are common to all S modeling routines.
The [[terms]] function takes a standard formula, and returns an object that
is used for later processing.
The [[specials]] argument asks the function to note any calls to
\emph{cluster} or \emph{strata} in the formula, this makes it possible
for us to pull out those terms for special processing.

The \emph{cluster()} function is used in [[coxph]] to
obtain a generalized estimating equation (GEE) type of variance estimate.
Random effects and GEE are two different ways to approach
correlated outcomes, but they cannot be mixed.  
Thus such a term is invalid in a [[coxme]] model.

In a Cox model the baseline hazard $\lambda_0$ plays the role of an
intercept, but the $X$ matrix does not explicitly contain an intercept.
Nevertheless, contrasts terms, such as the dummy variable codings for
factors, need to be formed as though there were an intercept term.
We thus mark the model as containing an intercept column by setting the
intercept attribute of [[terms]] (and completely ignore any ``-1'' that
the user put into the model) before calling [[model.matrix]].
After then remove the unneeded intercept column from the returned
matrix.  
The resulting $X$ matrix might have only one column; the [[drop=F]]
option causes it to remain a matix and not become a vector.
If there are only random effects in the model, $X$ could even have 0
columns.

If there are strata, they are removed from the model formula
before forming the X matrix, since strata effect only the
baseline hazard.
The variable [[strata.keep]] retains the strata levels as specified by
the user.
The variable [[strats]] has values of 1,2, \ldots and
is simpler for the underlying C code to deal with.

\subsection{Random effects}
Creating the random effects components is more complicated than the 
fixed effects.
\begin{itemize}
  \item We need to create both the $Z$ matrix and $\Sigma$.
  \item The actual form of $Z$ depends on the type of random
    effect: a familial correlation will be different than an compound
    symmetry.  There are many possible correlation structures.
  \item If there are multiple random terms, each creates a block of columns
    in $Z$ and block of values in $\Sigma$.
  \item For efficiency, some or all of the class variables in $Z$ may be
    represented in compressed form.  
    Such variables are stored in a matrix $F$ which has a single column 
    for each class variable, with integer
    values of 1,2, \ldots. $Z$ will contain the remaining
    columns.
\end{itemize}
The basic flow of the routine is to process the random terms one at a time.
The [[varlist]] component describes a variance family for each term; and
we do two calls for each.
The first call is to the \emph{init} member of the family, giving it the
$G$ containing the grouping variables along with the number of columns in
$C$ and whether or not the left hand side contained an intercept.
It returns corresponding columns of $F$ and $Z$, a named vector of parameters
$\theta$ containing the set of variance parameters to be optimized, and
a parameter list which will be used in the [[coxme.fit]] routine.
 
Given the names of the $\theta$ parameters, the routine is now able to match
any elements of the [[vinit]] and [[variance]] arguments to it, in order
to set either initial values or fixed values for them.
The second call is to the \emph{iparm} member of the family, which then sets
initial values for the remainder of the parameters, as well as possibly
transforming some or all of them to a different scale.  
Such a transformation is private to the variance family function.

The [[formula2]] function is desribed later; it is responsible for 
further separating the components of each random terms for us: 
whether the left hand side has
an intercept, any other variables on the left, grouping variables, and
optional interaction.

Our first action is to check out the [[varlist]] option.  This is 
complicated by the fact that users can give a partial one, or none,
allowing the default to be used for other elements.
Each element of the varlist should be one of the generating functions
for a [[coxvar]] object.
If it is not, then the arguments are 
turned into a call to [[coxvarMlist]], which
handles lists that contain a mixture of matrices and/or matrix
generating functions.
This latter is to maintain backwards compatability 
with the first coxme code, which allowed only that form.
Of course one problem with this is that a simple typing mistake will
lead to an error from within the [[coxvarMlist]] routine, which may
be more confusing.
<<build-control-structures>>=
nrandom <- length(flist$random)
if (nrandom ==0) stop("No random effects terms found")
<<get-cmat>>
<<get-groups>>
<<make-vinit>>
<<newzmat>>
vparms <- vector('list', nrandom)
fname <- zname <- thetalist <- vparms
if (missing(varlist)) {
    varlist <- vector('list', nrandom)
    for (i in 1:nrandom) varlist[[i]] <- coxvarFull #default
    }
else {
    if (nrandom==1) { # allow a single non-list
        if (!is.list(varlist)) varlist <- list(varlist)
        }
    if (length(varlist) != nrandom) stop ("Wrong length for varlist")
    for (i in 1:length(varlist)) {
        if (!inherits(varlist[[i]], 'coxvar'))
            varlist[[i]] <- coxvarMlist(varlist[[i]])
        }
    }
@ 
At this point we have a valid [[varlist]] object, with is a list with
one element per random term, each element is an object of class `coxvar'.
The current options for these elements are
\begin{description}
\item [coxvarFull] All variance/covariance terms between random elements are
present.  For instance the term [[(1+age | group)]] specifies a random
intercept and slope.  The variance structure will have 3 parameters: the variance of the intercepts, the
variance of the slopes, and their covariance.
\item [coxvarMlist]  The variance is assumed to be of the form
$\sigma_1^2 A_1 + \sigma_2^2 A_2 + \ldots$ for a set of fixed matrices
$A_1, A_2, \ldots$.  This is commonly used in genetic studies where $A_1$
would be the kinship matrix for a set of subjects/families and $A_2$ might
be the identity-by-descent (IBD) matrix for a particular locus.
For backwards compatability a naked list of matrices may be given, and they
are packaged into a call to [[coxvarMlist]]; however this does not allow a user
to modify certain options.
\end{description}

Now we proceed through the list one element at a time, and do the necessary
setup. The variables [[nfac]] and [[nslope]] contain the number of columns
of [[fmat]] and [[zmat]] contributed by each term. 

<<build-control-structures>>=    
fmat <- zmat <- NULL
nfac <- nslope <- integer(nrandom)
stemp <- sparse
for (i in 1:nrandom) {
    f2 <- formula2(flist$random[[i]])
    vfun <- varlist[[i]]
    if (!is.null(f2$interaction)) stop("Interactions not yet written")

    cmat <- getcmat(f2$fixed, m)
    groups <- getgroups(f2$group, m)
    init <- vfun$init(vinit[[i]], variance[[i]], intercept=f2$intercept, 
                        groups, cmat, stemp)
    vparms[[i]] <- init$parms

    if (f2$intercept) {
        if (!is.matrix(init$F) || nrow(init$F) !=n) 
            stop("Invalid result from coxvar function for F")
        nfac[i] <- ncol(init$F)
        fmat <- cbind(fmat, init$F)
        if (stemp[2] < 1) {
            nsparse <- init$sparse
            stemp[2] <- 1
            }
        }
    if (!is.null(cmat)) {
        temp <- newzmat(cmat, fmat)
        zmat <- cbind(zmat, temp)
        nslope[i] <- ncol(temp)
        }
} 
if (nsparse>0 & nfac[1]==0) { #must reorder
    # Fix this later
    stop("Only the first random term can be sparse")
    }
@ 
The matrix $F$ holds the columns associated with intercept terms,
so has columns added only if the new random terms has a 1 on the
left side of the formula.
It is also (at present) the only case in which sparse computation is
known to be valid.  However, only one term can be sparse:  once one
has been found the second element of stemp is set to 1 to ensure no
others.

 The underlying C programs can't deal with holes in a factor variable.
That is, every column of fmat must be integers, with minimum 1 and no
gaps.  We ensure this, at the same time remembering the number of
unique levels in each, which is what will determines the number of
random coefficients $b$.
<<build-control-structuer>>=
if (is.null(fmat)) nfac.nlevel <- NULL
else {
    nfac.nlevel <- integer(ncol(fmat))
    for (i in 1:ncol(fmat)) {
        if (any(fmat[,i] != as.integer(fmat[,i]))) 
            stop("Invalid code for group")
        temp <- sort(unique(fmat[,i]))
        if (temp != 1:(max(temp)))
            stop ("Missing code in group")
        nfac.nlevel[i] <- max(temp)
        }
    }


Now to fill in a few blanks from the above discussion.
First the vector (list) of initial values [[vinit]] or fixed variance
values [[variance]] given by the user may not be complete.
We want to expand them out to to lists, and have the same length as 
[[varlist]].
In the case of multiple terms, we allow the user to specify a subset of
them, using the names of the grouping variables.  If names are
not used (or are not unique) things match in order.
If there is a single random term we allow a numeric vector.
<<make-vinit>>=
if (missing(vinit)) vinit <- vector('list', nrandom)
else {
    if (nrandom==1 && is.numeric(vinit)) vinit <- list(vinit)
    if (!is.list(vinit)) stop("Invalid value for `vinit` parameter")
    if (length(vinit) > nrandom) stop ("Invalid length for vinit")
    if (!all(sapply(vinit, is.numeric))) 
        stop("Vinit must contain numeric values") 
    
    if (length(vinit) < nrandom) 
        vinit <- c(vinit, vector('list', nrandom - length(vinit)))
                   
    tname <- names(vinit)
    if (!is.null(tname)) {
        temp <- pmatch(tname, names(flist$random), nomatch=0)
        temp <- c(temp, (1:nrandom)[-temp])
        vinit <- vinit[temp]
        }
  }

if (missing(variance)) variance <- vector('list', nrandom)
else {
    if (nrandom==1 && is.numeric(variance)) variance <- list(variance)
    if (!is.list(variance)) stop("Invalid value for `variance` parameter")
    if (length(variance) > nrandom) stop ("Invalid length for variance")
    if (!all(sapply(variance, is.numeric))) 
        stop("Variance must contain numeric values") 
    
    if (length(variance) < nrandom) 
        variance <- c(variance, vector('list', nrandom - length(variance)))
                   
    tname <- names(variance)
    if (!is.null(tname)) {
        temp <- pmatch(tname, names(flist$random), nomatch=0)
        temp <- c(temp, (1:nrandom)[-temp])
        variance <- variance[temp]
        }
  }
@ 

The actual computation of the model is done in [[coxme.fit]].  
This was separated from the main routine partly to leave the code in
managable chunks.
<<call-computation-routine>>=
browser()
fit <- coxme.fit(X, Y, strats, offset, init, control, weights=weights,
                 ties=ties, row.names(m), refine.n,
                 varlist, vparm, thetalist, fixed, 
                 fmat, zmat, refine.n)
@ 

Then we finish up be packaging up the results for a user.
The first few lines are the case where a fatal error occured, in which
case the result contains only the failure line.
(Is this needed?)
<<finish-up>>=
if (is.character(fit)) {
    fit <- list(fail=fit)
    oldClass(fit) <- 'coxme'
    return(fit)
    }
@ 
Now add labels to the fixed coefficients and to the frailty terms.
The [[coefficients]] portion of the returned object conttains the
values for $\hat \beta$ (fixed) and for the variances $\hat \theta$.
The [[frail]] component contains the values for $\hat b$.
<<finish-up>>=
time2 <- proc.time()
fcoef <- fit$coefficients$fixed
nvar <- length(fcoef)
if (length(fcoef)>0 && any(is.na(fcoef))) {
    vars <- (1:length(fcoef))[is.na(fcoef)]
    msg <-paste("X matrix deemed to be singular; variable",
                    paste(vars, collapse=" "))
    if (singular.ok) warning(msg)
    else             stop(msg)
    }
if (length(fcoef) >0) {
    names(fcoef) <- dimnames(X)[[2]]
    fit$coefficients <- list(fixed=fcoef, random=fit$coeff$random)
    }

if (ncluster==1) {
    names(fit$frail) <- dimnames(varlist[[1]][[1]])[[1]]
    flinear <- fit$frail[kindex]
    }
else {
    ftemp <- vector('list', ncluster)
    j <- 0
    flinear <- 0
    for (i in 1:ncluster) {
        tname <- dimnames(varlist[[i]][[1]])[[1]]
        nf <- length(tname)
        temp <- fit$frail[j + 1:nf]
        flinear <- flinear + temp[kindex[,i]]
        names(temp) <- tname
        ftemp[[i]]<- temp
        j <- j+nf
        }
    names(ftemp) <- gnames
    fit$frail <- ftemp
    }

if (nvar ==0) fit$linear.predictor <- as.vector(flinear)
else fit$linear.predictor <- as.vector(flinear + c(X %*% fit$coef$fixed))
@ 
Last fill in a set of miscellaneous members of the structure
<<finish-up>>=
fit$n <- nrow(Y)
fit$terms <- Terms
fit$assign <- attr(X, 'assign')

na.action <- attr(m, "na.action")
if (length(na.action)) fit$na.action <- na.action
if (x)  {
    fit$x <- X
    if (length(strats)) fit$strata <- strata.keep
    }
if (y)     fit$y <- Y
if (!is.null(weights) && any(weights!=1)) fit$weights <- weights

time3 <- proc.time()
timeused <- c((time1[1]+ time1[2]) - (time0[1] + time0[2]), fit$timeused,
              (time3[1]+ time3[2]) - (time2[1] + time2[2]))
timeused <- c(sum(timeused), timeused)
names(timeused) <- c("Total", "setup", "fit1", "fit2", "fit3", "finish")
fit$timeused <- timeused

fit$formula <- as.vector(attr(Terms, "formula"))
fit$call <- Call
fit$ties <- ties
fit$kindex <- kindex
names(fit$loglik) <- c("NULL", "Integrated", "Penalized")
oldClass(fit) <- 'coxme'
fit
}
@ 

\subsection{Creating the $C$ and $F$ matrices}
To create the columns for $F$ there are 3 steps.
First we get the variables from the data frame, treating each of them
as a factor.  This is then submitted to the
appropriate coxvar family function, which
creates the integer matrix of codes that are actually used.

The allnames function will be given a single right-hand side of a
single random effects term, so it does not need to be fancy.
(Question - is there  a standard R function for this?)
<<get-groups>>=
allnames <- function(x){
    if (is.call(x)) {
        if (length(x) == 3) return( c(allnames(x[[2]]), allnames(x[[3]])))
        else return(allnames(x[[2]]))
      }
    if (is.name(x)) as.character(x) else NULL
  }

getgroups <- function(x, mf) {
    varname <- allnames(x)
    if (is.null(varname)) return(NULL)  # a shrinkage effect like (x1+x2 | 1)
    data.frame(lapply(mf[varname], as.factor))
    }
@ 
A common task for the variance functions is to exand [[school/teacher]]
type terms into a set of unique levels, i.e., to find all the unique
combinations of the two variables.  Teacher 1 in school 1 is not the same
person as teacher 1 in school 2.
We can't use the usual processing functions such as [[model.matrix]]
to create the nesting variables, since it also expands the factors
into multiple columns of a matrix.  (This is how [[lmer]] does it.)
We will use the [[strata]] function from the standard survival library.
<<expand.nested>>=
expand.nested <- function(x) {
    if (length(x) >1) {
        for (i in seq(2, length(x), by=1)) {
            x[[i]] <- strata(x[,i-1], x[i], shortlabel=TRUE, sep='/')
            }
       } 
    x
    }
@     

Creation of the $C$ matrix is just a bit more work.  
One issue is that none of the standard S contrast options is correct.
With a Gaussian random effect, either a random intercept or a random slope,  
the proper constraint is $b' \Sigma =0$;
this is familiar from older statistics textbooks for ANOVA as the ``sum 
constraint''. 
For a random effect this constraint is automatically enforced by the 
penalized optimization, so the proper coding of a factor with $k$ levels
is as $k$ indicator variables.
We do this by imposing our own contrasts. 
The formula inherited from [[formula2]] already has any intercept removed.
<<get-cmat>>=
getcmat <- function(x, mf) {
    if (is.null(x)) return(NULL)
    varname <- allnames(x)
    m2 <-  mf[,varname]
    for (i in 1:ncol(m2)) {
        if (is.factor(m2[[i]])) {
            nlev <- length(levels(m2[[i]]))
            contrasts(m2[[i]], nlev) <- diag(nlev)
            }
        }
    model.matrix(terms(x), m2)
    }
@ 

The $C$ matrix is crossed with the random effects to get the $Z$
matrix $Z=(C_{11}, C_{2}, \ldots, C_{21}, \ldots)$,
where $C_{ij}$ is [[cmat * (fmt[,i]==j])]]. 
If $F$ has multiple columns and/or any one of the columns has a lot of 
levels then $Z$ can get very large.  
Additionally, if $Z$ has $p$ columns then the Hessian matrix 
for the corresponding
parameters is $p$ by $p$.  
This is an area where the code could use more sparse matrix intelligence, i.e.,
so that the expanded $Z$ need never be created.

<<newzmat>>=
newzmat <- function(cmat, fmat) {
    newcol <- ncol(cmat) * sum(apply(fmat,2,max))
    newz <- matrix(0., nrow=nrow(cmat), ncol=newcol)
    indx <- 0
    nc <- ncol(cmat)
    for (i in 1:ncol(fmat)){
        for (j in 1:max(fmat[,i])) {
            newz[, 1:nc + indx] <- cmat * (fmat[,i]==j)
            indx <- indx + nc
            }
        }
    newz
    }
@ 
\section{The model formula}
\subsection{Introduction}
The first version of [[coxme]] followed the [[lme]] convention of
using separate formulas for the fixed and random portions of the
model.
This worked, but has a couple of limitations.
First, it has always seemed clumsy and unintuitive.  
A second more important issue is that it does not allow for
variables that participate in both the fixed and random effects. 
The new form is similar (but not identical) to the direction
taken by the [[lmer]] project.  
Here is a moderately complex example modivated by a multi-institutional
study where we are concerned about possible different patient
populations (and hence effects) in each enrolling institution.

\begin{verbatim}
   coxme(Surv(time, status) ~ age + (1+ age | institution) * strata(sex))
\end{verbatim}
This model has a fixed overall effect for age, along with random
intercept and slope for each of the enrolling institutions.
The study has a separate baseline hazard for males and females, along
with an interaction between strata and the random effect.
The fitted model will include separate estimates of the variance/covariance
matrix of the random effects for the two genders.
This is a type of model that could not be specified in the prior mode
where fixed and random effects were in separate statements.


\subsection{Parsing the formula}
The next step is to decompose the formula into its component parts, 
namely the fixed and the random effects.
The standard formula manipulation tools in R are not up to this
task; we do it ourselves using primarily two routines called,
not surprisingly [[formula1]] and [[formula2]].  
The first breaks the formula into fixed and random components,
where the fixed component is a single formula and the random
component may be a list of formulas if there is more than one
random term.

\begin{figure}
\myfig{figtree1}
\caption{The parse tree for [[y ~ x1 + (x3 + x4)* x2]].}
\label{figtree1}
\end{figure}

Formulas in S are represented as a parse tree.  For example, consider
the formula [[y ~ x1 + x2*(x3 + x4)]].
It's tree is shown in figure \ref{figtree1}.
At each level the figure lists the class of the object along with its name;
to lessen crowding in the plot objects of class `name' do not have the class
listed.
The arguments to a call are the branched below each call.
A formula is structured like a call to the `\verb2~2' operator, and a
parenthesised expression like a call with a single argument.

The [[formula1]] routine is called with the model formula,
the response and the fixed parts are returned as the [[fixed]]
component, the random parts are separated into a list.  
The primary concern of this function is to
separate out the random terms;
by definition this is a parenthesised term whose first child in the
parse tree is a call to the vertical bar function.
A random term is separated from the rest of the equation by one of
the four operators +, -, *, or :,
thus the parsing routine only has to worry about those four,
anything else can safely be lumped into the fixed part of the 
equation.  

We first deal with the top level call (the formula), and with
parentheses.
There are two cases.  In the first, we have by definition found
a random effects term.  
(The routine [[formula2]] will be used to check each random
term for validity later).
The second case is a random term found inside two sets of parentheses;
this is redundant but legal.  By simply passing on the list from the 
inner call the routine removes the extra set.

<<formula>>=
formula1 <- function(x) {
    if (class(x)=='formula') {  #top level call
        n <- length(x)  # 2 if there is no left hand side, 3 otherwise
        temp <- formula1(x[[n]])
        if (is.null(temp$fixed)) x[[n]] <- 1  # only a random term!
        else x[[n]] <- temp$fixed
        return(list(fixed=x, random=temp$random))
        }
    
    if (class(x) == '(' ) {
        if (class(x[[2]])== 'call' && x[[2]][[1]] == as.name('|')) {
            return(list(random = list(x)))
            }
            
        temp <- formula1(x[[2]])  # look inside the parenthesised object
        if (is.null(temp$fixed)) return(temp) #doubly parenthesised random 
        else {
            # A random term was inside a set of parentheses, pluck it out
            #  An example would be (age + (1|group))
            if (length(temp$fixed) <= 2) x <- temp$fixed  #remove unneeded (
            else      x[[2]] <- temp$fixed
               return(list(fixed= x, random=temp$random))
            }
        }
@ %$
Next we deal with the four operators one by one, starting with `+'.
We know that this call has exactly two arguments; 
the routine recurs on the left and then the right hand portions, and
then merges the results.  
The merger has to deal with 4 cases, the left term either did or did not
have a fixed effect term, and the right either did or did not.  
We re-paste the two fixed effect portions together.  
The random terms are easier since they are lists, which concatonate
properly even if one of them is null.
<<formula>>=
    if (class(x) == 'call' && x[[1]] == as.name('+')) {
        temp1 <- formula1(x[[2]])
        temp2 <- formula1(x[[3]])

        if (is.null(temp1$fixed)) {
            # The left-hand side of the '+' had no fixed terms
            return(list(fixed=temp2$fixed, 
                        random=c(temp1$random, temp2$random)))
            }
        else if (is.null(temp2$fixed)) # right had no fixed terms
            return(list(fixed=temp1$fixed, 
                        random=c(temp1$random, temp2$random)))
        else {
            return(list(fixed= call('+', temp1$fixed, temp2$fixed),
                        random=c(temp1$random, temp2$random)))
            }
        }
@  
The code for `-' is identical except for one extra wrinkle: you cannot
have a random term after a minus sign.
Becase the expressions are parsed from left to right [[~ age-1 + (1|group)]]
will be okay (though -1 makes no sense in a Cox model),
but [[~ age - (1 + (1|group))]] will fail.  
<<formula>>=
    if (class(x)== 'call' && x[[1]] == as.name('-')) {
        temp1 <- formula1(x[[2]])
        temp2 <- formula1(x[[3]])
        if (!is.null(temp2$random))
            stop("You cannot have a random term after a - sign")

        if (is.null(temp1$fixed))  #no fixed terms to the left
            return(list(fixed=temp2$fixed, 
                        random= temp1$random))
        else {  #there must be fixed terms to the right
            return(list(fixed= call('-', temp1$fixed, temp2$fixed),
                        random= temp1$random))
            }
       }            
@
For the last line: we know there is something to the right of the '-', and
it is not a naked random effects term, so it must be fixed.

Interactions are a bit harder.  The model formula
[[~ (age + (1|group))*sex]] for instance has an [[age*sex]] fixed term and
a [[(1|group)*sex]] random term.  
Interactions between random effects are not defined.
I don't know what they would mean if they were \ldots.
<<formula>>=
    if (class(x)== 'call' && (x[[1]] == '*' || x[[1]] == ':')) {
        temp1 <- formula1(x[[2]])
        temp2 <- formula1(x[[3]])

        if (is.null(temp1$random) && is.null(temp2$random))
            return(list(fixed=x))   # The simple case, no random terms

        if (!is.null(temp1$random) && !is.null(temp2$random))
                stop ("The interaction of two random terms is not defined")
@ 
Create the new `fixed' term.  In the case of [[(1|group):sex]], there is no
fixed term in the result.  
For [[(1|group) *sex]] the fixed term will be `sex'.
These are the two cases (and their mirror images) where only one of the left
or right parts has a fixed portion.
If both have a fixed portion then we glue them together.
<<formula>>=
        if (is.null(temp1$fixed) || is.null(temp2$fixed)) {
            if (x[[1]] == ':') fixed <- NULL
            else if (is.null(temp1$fixed)) fixed <- temp2$fixed
            else fixed <- temp1$fixed
            }
        else  fixed <- call(deparse(x[[1]]), temp1$fixed, temp2$fixed)
@
% 
Create the new random term.  The lapply is needed for
[[(((1|group) + (1|region)) * sex]], i.e., there are multiple groups
in the random list.
I can't imagine anyone using this, but if I leave it out they surely
will and confuse the parser.
<<formula>>= 
        if (is.null(temp2$random))  #left hand side was random
            random <- lapply(temp1$random, 
                             function(x,y) call(':', x, y), y=temp2$fixed)
        else  #right side was
            random = lapply(temp2$random,
                                 function(x,y) call(':', x, y), y=temp1$fixed)

        if (is.null(fixed)) return(list(random= random))
        else return(list(fixed=fixed, random=random))
        }
@ 
The last bit of the routine is for everything else, we treat it as a 
fixed effects term.
A possible addition would be look for any vertical bars, which by definition
are not a part of a random term --- we've already checked for parentheses ---
and issue an error message.  We do this instead in the parent routine.
<<formula>>=
    return(list(fixed=x))
}
@ 


\subsection{Random terms}
Each random term is subjected to further analysis using the
[[formula2]] routine.  
This has a lot of common code with [[formula1]], since they both
walk a similar tree.  
The second routine breaks a given random part into up to four parts,
for example the result of [[(1 + age + weight | region):sex]]
will be a list with elements:
\begin{itemize}
  \item [[intercept]]:  TRUE
  \item [[variates]]: age + weight
  \item [[group]]: region
  \item [[interaction]]: sex
\end{itemize}

We can count on [[formula1]] to have put any interaction term on the
far right, which means that it will be the first thing we encounter.
<<formula>>=
formula2 <- function(term) {
    if (is.call(term) && term[[1]] == as.name(':')) {
        interact <- term[[3]]
        term <- term[[2]]
        }
    else interact <- NULL
   
    if (class(term) != '(' || !is.call(term[[2]]) || 
                              term[[2]][[1]] != as.name('|')) 
        stop("Formula error: Expected a random term") 

    term <- term[[2]]  # move past the parenthesis
    out <- findIntercept(term[[2]])
    out$group<- term[[3]]
    out$interaction <- interact
    out
  }
@ 

This routine looks for an intercept term - that's all.
It's easiest to use the built in [[terms]] function for this,
since the intercept could be anywhere, and someone might have put
in a -1 term which makes it trickier.  
We turn the formula fragment back into a real formula, as required
by the terms funcion, by a call to the tilde function.
This will change the formula [[x*z]] into [[x + z + x:z]], but I don't
really care since the fixed portion is only going to be used as input
into [[model.matrix]].
<<formula>>=
findIntercept <- function(x) {
   tt <- terms(eval(call('~',x)))
   if (length(attr(tt, 'term.labels')) > 0) {
       temp <- paste(attr(tt, 'term.labels'), collapse= '+')
       list(intercept= (attr(tt, 'intercept') ==1),
            fixed = formula(paste('~', temp)))
       }
   else list(intercept= (attr(tt, 'intercept') ==1))
 }
@ 

\subsection{Miscellaneous}
Here is the simple function to look for any vertical bars.
You might think of recurring on any function with two arguments,
e.g., [[if length(x)==3]] on the fourth line.  
(The [[findbars]] routine in lmer, 3/2009, does this for instance, which
shows that it must be a pretty sound idea, given the extensive use
that code has seen.)
However, that line would recur into other functions, like [[logb(x5, 2)]]
for instance.  I've never seen of an instance where this would lead
us to a bar that we ``shouldn't see'', but still I'm taking the paranoid
route.  
Looking inside single parameter functions likely would get us caught
someday with [[I(x1 | x2)]], which is certainly legal.

<<formula>>=
hasAbar <- function(x) {
  if (class(x)== 'call') {
        if (x[[1]]== as.name('|')) return(TRUE)
        else if (x[[1]]==as.name( '+') || x[[1]]== as.name('-') ||
                 x[[1]]==as.name( '*') || x[[1]]== as.name(':'))
	    return(hasAbar(x[[2]]) || hasAbar(x[[3]]))
        else return(FALSE)
        }
    else if (class(x) == '(') return(hasAbar(x[[2]]))
    else return(FALSE)
    }
@ 

Here is a similar function which replaces each vertical bar with a '+'
sign.  This is needed for the [[model.frame]] call, which does not
properly deal with vertical bars. 
Given a formula it returns a formula.
We only recur on 4 standard operators to avoid looking inside functions. 
An example would be [[~ age + I(x1 | x2) + (1|group)]].
I'm not sure that replacing the bar inside the I() function will
cause any problems for model.frame; so I may be being overly cautious.
<<formula>>=
subbar <- function(x) {
    if (class(x)=='formula') x[[length(x)]] <- subbar(x[[length(x)]])
    if (class(x)== 'call') {
        if (x[[1]]==as.name( '+') || x[[1]]== as.name('-') ||
            x[[1]]==as.name( '*') || x[[1]]== as.name(':')) {
	    x[[2]] <- subbar(x[[2]])
            x[[3]] <- subbar(x[[3]])
            }
        }
    else if (class(x)== '(') {
        if (class(x[[2]])== 'call' && x[[2]][[1]] == as.name('|')) 
            x[[2]][[1]] <- as.name('+')
        else x[[2]] <- subbar(x[[2]])
        }
    x
    }
   
@ 
    
    
\section{Variance families}
\subsection{Structure}
Each distinct random effects term corresponds to a distinct
diagonal block in the overall penalty matrix, along with a
set of penalized coefficients $b$.
To make life easier for the maximizer, there may also
be a transformation between the displayed variance 
coefficients and the internal ones.
When there are multiple random terms in the formula then the
[[varfun]], [[vinit]], and [[variance]] arguments must each be in
the form of a list with one element per random term. 

The variance family objects for [[coxme]] are similar in spirit to [[glm]]
families: the functions set up the structure but do not do any
work.  
Each of them returns a list of 3 functions, [[init]], [[generate]], and
[[wrapup]].
Any optional arguments to the variance family are used to create these
three; depending on the family they might apply to any one.

The init function is called with the $Z$ and $G$ matrices for the
given term, along with the sparse option and 
the appropriate vectors of initial and fixed values.

The return from a call to init is
\begin{description}
\item[theta] a vector of initial values for all the parameters
that need to be optimized.  This implicitly gives the number
of parameters to optimize.
\item[X] the design matrix for any random slopes
\item[G] the design matrix for any random factors
\item[parm] a list of arguments to be passed forward to the 
generate and wrapup functions.  
\end{description}

The $G$ matrix passed in and the $G$ matrix returned may not be the same.
In particular, any class levels that are going to be treated as
sparse will have been rearranged to so as to be the first columns of
the penalty matrix (variance of the random effect), and so will have
level indices of 1,2, \ldots.

The \emph{generate} function is called at each iteration with the current
vector of parameters $\theta$ and the parameter list.  It will generate
the variance matrix of the random effect. 
If there are multiple random effect terms, each of the generate
functions creates the appropriate block.

The \emph{wrapup} function is called when iteration is complete.  It's
primary job is to return the extended and re-transformed $\theta$
vector.  Fixed coefficients are re-inserted.

\subsection{coxvarFull}
This is the default routine, which assumes a simple nested structure
for the variance.
The only argument is \emph{collapse}, which determines the interal
representation of nested categories.
Assume that we have a classic nested categorical structure with
two levels; $i=1, \ldots, g$ for the $g$ groups and $j=1, \ldots, h$
for the $h$ subgroups within each of the $g$ groups.
\begin{eqnarray*}
 u_{ij}&=& b1_i + b2_{ij} \\
 b1 &\sim& N(0, \sigma_1^2) \\
 b2 &\sim& N(0, \sigma_2^2)
\end{eqnarray*}
This will lead to variables $b1$ with $g$ levels and $b2$ with $gh$
levels, two colunms in the design matrix $G$, and a diagonal variance
structure for these $g + gh$ variables.
An alternate structure is to consider a single random effect
$b3 \equiv b1 + b2$ which has $gh$ levels, and a block variance
stucture.  There will be $g$ blocks of size $h \times h$ having
a diagonal of $\sigma_1^2 + \sigma_2^2$ and off-diagonal elements
of $\sigma_2^2$.  
This latter is the collapsed form.
We have found that the maximizer has a much easier time when this is used.

The counterpoint to this is sparsity, where the collapsed form is
somewhat harder to deal with.
Note that any sparse coefficient must be first in the $b$ vector
and in the $G$ matrix.

The very first part of the routine is to pull off initial values and
fixed values.  This will be a part of every initial routine, but
unfortunately cannot be moved up the the caller, because the
caller [[coxme]] does not know how many $\theta$ values will be
used by each term.
For both, the user can give either a named vector, in which case
parameters are pulled off by name, or else a parameter vector
of the correct length.  In the latter case a value of 0 for fixed
means that the parameter is free.

<<coxvarFull>>=
coxvarFull <- function(collapse=TRUE) {
    collapse <- collapse
    # Because of R's lexical scoping, the values of the options
    #  above, at the time the line below is run, are known to the
    #  init function
    init <- function(init, fixed, Z, G, gnames, sparse) {
        ncluster <- length(gnames) # also = ncol(G)
        my.init <- rep(.2, ncluster)
        names(my.init) <- gnames
        my.fixed <- rep(0, ncluster)

        if (!missing(init)) {
            if (length(init) != ncluster) 
                    stop ("Wrong length for initial values")
            my.init<- init
            }

        if (!missing(fixed)) {
            if (length(fixed) != ncluster)
                stop ("Wrong length for fixed variance vector")
            my.fixed <- fixed
            }
@ 

Tne next task of the routine is to count up the total number of
groups, and from this the total number of parameters $\theta$
for the matrix.
The next part of the routine deals with the simple case of a single
grouping variable.  (I expect this to be the case for most calls).
If some terms can be considered sparse, then [[index]] will contain
the new encoding of the data; e.g., old group 10 may have become
group 1 so as to place the sparse groups first.  
The dense part of the matrix $R$ will contain any non-sparse terms
along with any random slope terms.
<<coxvarFull>>=
        if (is.null(Z)) nx <-0 else nx <- ncol(Z))
        if (ncluster==1) {
            temp <- table(G)
            ngroup <- length(temp) * (nx+1)
            nx2 <- nx* ngroup
            vmat <
            if (length(temp) >= sparse[1] && any(temp/sum(temp) < sparse[2])){
                which.sparse <- which(temp/sum(temp) < sparse[2])
                n.sparse <- length(which.sparse)
                index <- c(which.sparse, 1:ngroup[-which.sparse])
                n.dense <- ngroup - n.sparse
                if (n.dense > 1) {
                    vmat <- bdsmatrix(blocksize=rep(1, n.sparse),
                                      blocks=rep(1.0, nsparse),
                                  rmat=rbind(matrix(0., n.sparse, n.dense+nx2),
                                             diag(n.dense+ nx2)))
                    }
                else {
                    if (nx==0) vmat <- bdsmatrix(blocksize=rep(1, ngroup),
                                                 blocks = rep(1.0, ngroup))
                    else vmat <- bdsmatrix(blocksize=rep(1, ngroup),
                                           blocks = rep(1.0, ngroup),
                                           rmat= rbind(matrix(0., ngroup,nx2),
                                                       diag(nx2))
                }
            else {
                index <- 1:ngroup
                vmat <- bdsmatrix(blocksize=ngroup, blocks=diag(ngroup))
                }
          list(theta=my.init,  
                

                
        varmat <- vector(ncluster, 'list') 
        if (ncluster >1) {
            if (collapse) {
                g <- strata(G, shortlabel=TRUE)
                
                
            temp <- groups
            for (i in 2:ncluster)
                    temp[,i] <- strata2(groups[,1:i], shortlabel=shortlabel,
                                 sep='/')
            groups <- temp
            }
@ 
    
\subsection{coxvarMlist}
In a mixed-effects model the random effects $b$ are assumed to
follow a Gaussian distribution
$$
  b \sim N(0, \Sigma)
$$
In all the random effects modeling programs that I am aware of,
the user specifies the structure of $\Sigma$ and the program
constructs the actual matrix.  
For instance, `independent', `compound symmetry', or 'autoregressive'.
This basic approach does not work for genetic studies, since the
correlation is based on family structure and cannot be inferred from
a simple keyword.
The [[coxvarMlist]] variance specification accepts a list of
fixed matrices $A_1$, $A_2$, ... and fits the variance
structure $\Sigma = \sigma_1^2 A_1 + \sigma_2^2 A_2 + \ldots$.
The individual matrices are often in a block-diagonal sparse 
representation due to size.
(The motivating study for this structure had 26050 subjects with a
random intercept per subject, so that $A$ was 26050 by 26050.)

The matrices must have dimnames that match the levels of the 
grouping variable.  Much of the initialization work is to verify
this, remove unneeded columns of the matrices (if for instance a
subject has been dropped due to missing values), and reorder the
grouping variable to match the resulting matrix. ( Sparse matrices
cannot be arbitrarily reordered, so whatever label is on row 1 of the
variance matrix needs to become the first level of the grouping
variable, the second row the second, etc, during the computations.)
Much of this low level work is done by the [[coxme.varcheck]] routine.

Three checks on the matrices are commonly added.  
\begin{enumerate}
\item A solution with $A^*= A/2$ and $\sigma^* = \sigma \sqrt{2}$ is
of course equivalent to one with $A$ and $\sigma$.
For uniqueness, the matrices $A_1$, $A_2$ etc are rescaled to have a
diagonal of 1.  Kinship matrices in particular have a diagonal of 1/2.
\item The individual $A$ matrices are checked to verify that each is
positive definite.  If they are not this is most often reflects an 
error in forming them.
\item The parameters $\sigma$ are constrained to be $>0$.
\end{enumerate}
%I have had one problem where the first two had to be relaxed: to understand
%the interaction of gender and inheritance in breast/prostate cancer,
%we wanted to fit a model with the $n$ by $n$ kinship matrix $K$ as the
% correlation, but
%with separate scaling factors for male/male, male/female, and female/female
%pairs of subjects.
%This is easily done by a separation $K = K_{mm} + K_{ff} + K_{mf}$,
%where the $K_{mm}$ matrix for instance has zeros for all male/female and
%female/female elements.
%The $K_{mf}$ matrix has zero on the diagonal, so cannot be rescaled nor is
%it positive definite.

<<coxvarMlist>>=
coxvarMlist <- function(..., rescale=TRUE, pdcheck=TRUE,  positive=NULL) {
    varlist <- list(...)
    # Because of environments, the init function will inherit the
    #  three variables below
    rescale <- rescale
    pdcheck <- pdcheck
    positive <- positive
    
    init <- function(initial, fixed, intercept, G, Z, sparse) {
        ncluster <- length(G)
        if (ncluster==0) stop ("Mlist variance requires a grouping variable")
        if (length(Z)>0) stop ("Mlist variance does not allow random slopes")
        if (!intercept)  stop ("Mlist variance applies only to intercepts")

        groups <- expand.nested(G)
        temp <- coxme.varcheck(ncluster, varlist, n=length(G[[1]]),
                               gvars= names(G), 
                               groups= groups[[ncluster]], sparse,
                               rescale, pdcheck)
        ntheta <- temp$ntheta
        theta <- seq(.2, .3, length=ntheta) 
        if (length(initial)>0) {
            if (length(initial) != ntheta) 
                stop("Wrong length for initial vector")
            theta <- initial
            }
        if (length(fixed) >0) {
            if (length(fixed) != ntheta)
                stop("Wrong length for fixed values")
            which.fixed <- (!(is.na(fixed) | fixed==0))
            }
        else which.fixed <- rep(FALSE, ntheta)

        if (is.null(positive)) positive <- rep(TRUE, ntheta)
        else {
            if (!is.logical(positive))
                stop("Positivity constraint must be a logical vector")
            if (length(positive) != ntheta) 
                stop("Wrong length for positivity constraint")
            }
        if (any(positive & theta <=0))
            stop("Invalid initial value, must be positive")        
        theta[positive] <- log(theta[positive])

        list(F=temp$kindex, X=NULL, theta=theta[!which.fixed],
             parms=list(varlist =temp$varlist, theta=theta,
                        fixed=which.fixed, positive=positive))
        }
    
     generate <- function(newtheta, parms) {
         theta <- parms$theta
         theta[!parms$fixed] <- newtheta
         theta[parms$positive] <- exp(theta[parms$positive])
         
         varmat <- parms$varlist[[1]] * theta[1]
         if (length(theta) >1) {
             for (i in 2:length(theta)) {
                 varmat <- varmat + theta[i]*parms$varlist[[i]]
                 }
             }
         varmat
         }

    wrapup <- function(newtheta,parms) {
        theta <- parms$theta
        theta[!parms.fixed] <- newtheta
        theta[parms$positive] <- exp(theta[parms$positive])
        theta
        }
    
    out <- list(init=init, generate=generate, wrapup=wrapup)
    oldClass(out) <- 'coxvar'
    out
    }
@ 
\section{Fitting}
Consider the basic model
\begin{eqnarray*}
 \lambda(t) &=& \lambda_0(t) e^{X\beta + Zb} \\
 b &\sim& N(0, \Sigma(\theta))
\end{eqnarray*}

There are two sets of parameters.
The first is the set of regression coefficient $\beta$ and $b$,
the second is the vector $\theta$ that determines the variance
structure.
The basic structure of the iteration is
\begin{itemize}
\item an outer iteration process for $\theta$ which uses the standard
S routine [[optim]]
\item for any given realization of $\theta$ a computation of the optimal
values for $\beta$ and $b$
\begin{itemize}
\item S code is used to create the penalty matrix $\Sigma(\theta)$
\item C code solves for the regression coefficients, given $\Sigma$.
\end{itemize}
\end{itemize}

The overall outline of the routine is
<<coxme.fit>>=
 <<coxme-setup>>
 <<null-fit>>
 <<define-penalty>>
 <<coxme-fit>>
 <<coxme-finish>>
@ 

\subsection{Penalty matrix}
For the C code, the variance matrices of the individual
random effects are glued together into one
large bdsmatrix object $\Sigma$, [[kmat]] in the code; 
the inverse matrix $P = \Sigma^{-1}$ or 
[[ikmat]] is the penalty matrix of the computation,
is what is actually passed to C.
(The first large use of this code was for family correlation, where
$\Sigma$ is based on the \emph{kinship} matrix.  The variable names
[[kmat]] = $\Sigma$, [[ikmat]] for the inverse and [[kfun]] for
the calculation arise from this legacy.)
In order to make use of sparseness, 
the columns of [[kmat]] are expected to be in the following order
\begin{enumerate}
\item Random intercepts that are subject to sparse compuatation.
Only one random term is allowed to use sparse representation, i.e., the
first term in the model formula that has an intercept.
We have reordered the random terms, if necessary, to make it first in the list.
\item The remaining random intercepts
\item Other random coefficients (slopes)
\end{enumerate}
The overall coefficient vector has the random effects $b$ followed
by the fixed effects $\beta$,
with $b$ in the same order as the penalty matrix.

The key code chunk below creates kmat given the parameter vector $\theta$
([[theta]] for the non-mathematics types) and the variance list information.
Each of the [[generate]] functions creates a bdsmatrix consisting of a
block-diagonal-sparse portion and a dense portion; however all terms but
the first will have only a dense portion.
Remember that [[kmat]] is added to the Cox model's second derivative matrix
(hessian) which is nowhere sparse. 
So even if some parts of the constructed matrix $R$ below are zero, we keep
them so as to be conformant to the hessian.
If there is a single random term then this routine becomes very short
<<define-penalty>>=
whichterm <- rep(1:nrandom, nfac)  # which term does each column of fmat go to
kfun <- function(theta, varlist, parmlist, 
                 fcount=tapply(nfac.nevel,whichterm, sum), 
                 zcount = nslope) {

    nrandom <- length(varlist)
    if (nrandom == 1) return(varlist[[1]]$generate(theta, parmlist[[1]]))
@ 
If there is more than one, then we have some nitpicky bookkeeping.  Not
particularly hard but a nuisance.
Say for example that there are 3 terms with the following structure
$$\begin{tabular}{rccc}
& sparse & non-sparse \\
& intercept & intercept & covariate \\ \hline
term 1 & 60 & 2 & 64 \\
term 2 &  0 & 5 & 0 \\
term 3 &  0 & 8 & 16 \\
\end{tabular}
$$
This corresponds to [[~ (1+x | g1) + (1|g2) + (1+ z1 + z2 | g3)]]
where [[g1]] has two common and 60 uncommon levels; 
a complicated random effects model I admit.
The corresponding [[@rmat]] slots for the three [[bdsmatrix]] objects will
be of dimension $T=124\times 64$, $U=5 \times 5$ and $V=24 \times 24$.
The final bdsmatrix will have the block-diagonal portions from the first
and an overall right-hand side matrix of the form
$$
R= \left( \begin{array}{ccccc}
             T[1:62, 1:2] & 0 & 0 &0 &0\\
              0 & U[1:5,1:5] &0 & 0 & 0 \\
              0 & 0 & V[1:8, 1:8] &0 & 0 \\
              0 & 0 & 0 & T[63:128, 63:128] & 0 \\
              0 & 0 & 0  0 & V[9:24,9:24] \\ \end{array} \right)
$$
<<define-penalty>>=
    # Need to build up the matrix by pasting up a composite R
    nrow.R <- sum(fcount) + sum(zcount)
    ncol.R <- nrow.R - nsparse
    R <- matrix(0., nrow.R, ncol.R)
    
    fcount2 <- fcount; fcount2[1] <- fcount[1] - nsparse  
    indx1 <- cumsum(c(0, fcount2))  #offsets for intercept columns
    indx2 <- cumsum(c(0, fcount))   #offsets for intercept  rows
    indx3 <- cumsum(c(sum(fcount2), zcount)) # offsets for slope cols
    indx4 <- indx3 + nsparse  #     #offsets for slope rows
    for (i in 1:nrandom) {
        temp <- varlist[[i]]$generate(theta, parmlist[[i]])
        if (!inherits(temp, 'bdsmatrix')) 
            stop("penalty matrix for term", i, "must be a bdsmatrix")
        if (any(dim(temp)) != rep(fcount[i] + zcount[i],2))
            stop ("Invalid penalty matrix for term", i)
        if (fcount2[i] >0){
            t1 <- 1:fcount2[i]; t2 <- 1:fcount[i]
            R[t2 + indx2[i], t1+indx1[i]] <- temp@rmat[t2, t1]
            }
        if (zcount[i] >0) {
            t1 <-  1:zcount[i];  t2 <- t1 + fcount2[i]
            R[indx4[i] + t1, indx3[i]+t1] <- temp@rmat[t2, t2]
            }
        if (i==1) tsave <- temp
        }
    
    bdsmatrix(blocksize=tsave@blocksize, blocks=tsave@blocks, R=R)
    }    
@ 

\subsection{C routines}
The C-code underlying the computation is broken into 3 parts.
This was done for memory efficiency; due to changes in R and
S-Plus over time it may not as wise an idea as I once thought,
this is an obvious area for future simplification.

The initial call passes in the data, which is then copied to
local memory (using calloc, not under control of S memory
management) and saved.  The parameters of the call are
\begin{description}
  \item[n] number of observations
  \item[nvar] number of fixed covariates in X 
  \item[y] the matrix of survival times.  It will have 2 columns for normal
    survival data and 3 columns for (start, stop) data
  \item[x] the concatonated Z and X matrices
  \item[offset] vector of offsets, usually 0
  \item[weights] vector of case weights, usually 1
  \item[newstrat] a vector that marks the end of each stratum.  If for 
    instance there were 4 strata with 100 observations in each, this vector
    would be c(100,200,300,400); the index of the last observation in each.
  \item[sorted] A matrix giving the order vector for the data.  The first 
    column orders by strata, time within strata (longest first), and status
    within time (censored first).  For start, stop data a second column orders
    by strata, and entry time within strata. The -1 is because subscripts 
    start at 1 in S and 0 in C.
  \item[fmat] matrix containing the indices for random intercepts.  This has
    been preprocessed so that each column has coefficient indices into b
    rather than the original 1,2, \ldots codes.
  \item[findex] a 0/1 matrix with one column for each of fcol and nfrail
    rows, which marks which coefficients of $b$ are a part of that set.
    (A bookkeeping array for the C code that is easier to create here.)
  \item[$P$] some paramters of the bdsmatrix representing the penalty  
\end{description}
The other control parameters are fairly obvious.  From this data the C
routine can compute the total number of penalized terms and the number that
are sparse from the structure of the bdsmatrix, and the total number of
intercept terms as max(fmat).  Other dimensions follow from those.
A dummy call to [[kfun]] gives the necessary sizes for the penalty matrix.
All columns of the stored (Z,X) matrix are centered and scaled.

<<coxfit6a-call>>=
dummy <- kfun(theta, varlist)
if (is.null(dummy@rmat)) rtemp <- 0
    else                 rtemp <- ncol(dummy@rmat)

for (i in 2:ncol(fmat)) fmat[,i] <- fmat[,i] + max(fmat[,i-1])
findex <- matrix(0, nrow=max(fmat), ncol=ncol(fmat))
for (i in 1:ncol(fmat)) findex[cbind(fmat[,i], i)] <- 1
    
ifit <- .C('coxfit6a', 
               as.integer(n),
               as.integer(nvar),
               as.integer(ncol(y)),
               as.double(c(y)),
               as.double(x),
               as.double(offset),
               as.double(weights),
               as.integer(length(newstrat)),
               as.integer(newstrat),
               as.integer(sorted-1),
               as.integer(ncol(fmat)),
               as.integer(fmat-1),
               as.integer(findex),
               as.integer(length(dummy@blocksize)),
               as.integer(dummy@blocksize),
               as.integer(rtemp),
               means = double(temp.nvar),
               scale = double(temp.nvar),
               as.integer(ties=='efron'),
               as.double(control$toler.chol),
               as.double(control$eps),
               as.integer(control$sparse.calc))
    means   <- ifit$means
    scale   <- ifit$scale
@ 

The second routine does the real work and is called within the [[logfun]]
function, which is the minimization target of [[optim]]. 
The function is called with a trial value of the variance parameters
$\theta$, and computes the maximum likelihood estimates of $\beta$ and $b$
for that (fixed) value of $\theta$, along with the penalized partial
likelihood.  
The normalization constants include the determinant of [[kmat]], but since
we are using Cholesky decompositions this can be read off of the diagonal.
Hopefully the coxvar routines have chosen a parameterization that will
mostly avoid invalid solutions, i.e., those where [[kmat]] is not
symmetric positive definite.

The [[init]] parameter is a vector of starting estimates for the 
$(b, \beta)$ solution.  It is tempting to use the final results from
the prior iteration, but the solution from a model with no random effects
seems to be safer.  The (1 -fit0) addition makes the solution be in the
neighborhood of 1, which works well with the convergence criteria of
the [[optim]] routine.
We also found that it is best to always do the same number of iterations at
each call.  Changes introduce little 'bumps' into the apparent loglik, which
drives [[optim]] nuts. Hence the min and max iteration count is identical.

There are actually two C routines `coxfit6b' and 'agfit6b', for ordinary
and (start,stop) survival data, respectively.  The [[ofile]] argument 
is a character string giving the choice.
<<define-logfun>>=
logfun <- function(theta, varlist, parmlist, kfun,
                   init, fit0, iter, ofile) {
    gkmat <- gchol(kfun(theta, varlist, parmlist))
    ikmat <- solve(gkmat)  #inverse of kmat, which is the penalty
    if (any(diag(ikmat) <=0)) { #Not an spd matrix
        return(0)  # return a "worse than null" fit
        }
    fit <- .C(ofile,
              iter= as.integer(c(iter,iter)),
              beta = as.double(init),
              loglik = double(2),
              as.double(ikmat@blocks),
              as.double(ikmat@rmat),
              hdet = double(1))
    ilik <- fit$loglik[2] -
             .5*(sum(log(diag(gkmat))) + fit$hdet)

    -(1+ ilik - fit0)
    }
@ 


The third routine is used for iterative refinement of the Laplace
estimate. The arguments in this case are
\begin{description}
  \item[rfun] either 'coxfit6d' or 'agfit6d'
  \item[beta] the final solution vector or random effects $(b, \beta)$
  \item[gkmat] the final variance-covariance matrix at the solution
  \item[bmat] a matrix normal 0/1 random variables with nfrail rows (dimension
    of beta and of gkmat) and refin.n columns.  
    Each column of the product [[gkmat %*% bmat]] is a random Gaussian draw
    with correlation [[kmat]].
  \item[loglik] the actual log-likelihoods at the random points
  \item[approx] the Laplace approx at the random points
\end{description}
The routine simulates the difference between the true and Laplace approximation 
integrands.  

<<refine>>=
if (refine.n > 0) {
    bmat <- matrix(rnorm(length(beta)*refine.n), ncol=refine.n)
    sim.pen <- colSums(bmat^2)/2   #This is b' \Sigma^{-1} b /2 

    rfit <- .C(rfun,
               as.integer(refine.n),
               as.double(beta),
               as.double(gkmat %*% bmat),
               loglik = double(refine.n),
               approx = double(refine.n))
    errhat <- exp(rfit$loglik- ilik) - 
                      exp(fit$loglik[2] + sim.pen - (rfit$approx + ilik))
    ilik = ilik + log(1 + mean(errhat))
    r.correct <- c(correction=mean(errhat), var=var(errhat)/refine.n)
    }
@ 

The final routine [[coxfit6c]] is used for cleanup, and is
decribed in section \ref{sect:final}.

\subsection{Setup}
Preliminaries aside, let's now build the routine.
The input arguments are as were set up by [[coxme]], this
routine would never be called directly by a user.
\begin{description}
  \item[x] the matrix of fixed effects
  \item[y] the survival times, an object of class 'Surv'
  \item[strata] strata vector
  \item[offset] vector of offsets, usually all zero
  \item[control] the result of a call to coxme.control
  \item[weights] vector of case weights. usually 1
  \item[ties] the method for handling ties, 'breslow' or 'efron'
  \item[rownames] needed for labeling the output, in the rare case that
    the X matrix is null.
  \item[fmat] matrix of random factor (intercepts) indices.  If fmat[4,2]=6,
    this means that observation 4 is in the 6th level of the second
    grouping variable.
  \item[zmat] the Z matrix, the design matrix for random slopes
  \item[varlist] the list describing the structure of the random effects
  \item[theta] initial values for the random effects, e.g., the ones we need
    to solve for  (may be null if the variances are all fixed)
  \item[ntheta] vector giving the number of thetas for each random term
  \item[refine.n] number of iterations for iterative refinement
\end{description}
<<coxme-setup>>=
coxme.fit <- function(x, y, strata, offset, control,
			weights, ties, rownames, 
			fmat, zmat, varlist, theta, ntheta,
                        refine.n) {
    time0 <- proc.time()

    n <-  nrow(y)
    if (length(x) ==0) nvar <-0
    else nvar <- ncol(as.matrix(x))
    
    if (missing(offset) || is.null(offset)) offset <- rep(0.0,n)
    if (missing(weights)|| is.null(weights))weights<- rep(1.0,n)
    else {
	if (any(weights<=0)) stop("Invalid weights, must be >0")
	}
@ 
The next step is to get a set of sort indices, but not to actually
sort the data.  This was a key insight which allows the (start,stop)
version to do necessary bookkeeping in time of $(2n)$ instead of $O(n^2)$.
We sort by strata, time within strata (longest first), and status within
time (censor before deaths).  For (start, stop) data a second index 
orders the entry times.
<<coxme-setup>>=
    if (ncol(y) ==3) {
	if (length(strata) ==0) {
	    sorted <- cbind(order(-y[,2], y[,3]), 
			    order(-y[,1]))
	    newstrat <- n
	    }
	else {
	    sorted <- cbind(order(strata, -y[,2], y[,3]),
			    order(strata, -y[,1]))
	    newstrat  <- cumsum(table(strata))
	    }
	status <- y[,3]
        ofile <-  'agfit6b'
        rfile <-  'agfit6d'
        }
    else {
	if (length(strata) ==0) {
	    sorted <- order(-y[,1], y[,2])
	    newstrat <- n
	    }
	else {
	    sorted <- order(strata, -y[,1], y[,2])
	    strata <- (as.numeric(strata))[sorted]
	    newstrat <-  cumsum(table(strata))
	    }
	status <- y[,2]
        ofile <- 'coxfit6b' # fitting routine
        rfile <- 'coxfit6d' # refine.n routine
        }
@ 

Now for a bunch of checks.  These should not be necessary, since
the [[coxme]] routine has done it all.  But I'm sometimes
suspicious of my own code, and an extra check doesn't hurt.

<<coxme-setup>>=
    if (!is.list(varlist)) stop("variance matrix list isn't a list!")
    if (is.matrix(fmat) && ncol(fmat >1)) {
	ncluster <- ncol(fmat)
	clnames <- dimnames(fmat)[[2]]
	}
    else ncluster <- 1
    if (ncluster != length(varlist))
        stop("Lengths of variance list and of fmat disagree")

    #
    # Check out fmat:
    #   each column should be integer
    #   each col must contain numbers from 1 to k for some k
    #   column 1 should contain at least one "1" (the C routine
    if (any(fmat != floor(fmat))) stop("fmat must be integers")
    if (any(fmat <1)) stop("fmat must be >0")
    fmat <- as.matrix(fmat)
    nfrail <- apply(fmat, 2, max)
    temp <- apply(as.matrix(fmat), 2, function(x) length(unique(x)))
    if (any(nfrail != temp)) 
	stop("fmat must be a set of integer indices, with none missing")    
@ 

Now set up the C code
<<coxme-setup>>=
 <<coxfit6a-call>>
@ 
The last step of the setup is to do an initial fit.  
We want two numbers: the loglik for a no-covariate no-random-effects
fit, and that for the best fixed effects fit.
The first is the NULL model loglik for the fit as a whole, the second
is used to scale the logliklihood during iteration, the [[fit0]] parameter
in the [[logfun]] function.
The easiest way to get these is from an ordinary [[coxph]] call.
<<null-fit>>=
fit0 <- coxph(y ~ x + offset(offset), weights=weights, method=ties)
@ 

\subsection{Doing the fit}
If there are any paramters to optimize over, we do so.
Below optpar is a list of control parameters for the [[optim]] function,
which are defined in [[coxme.control]] and accessible for the user to
change, and [[logpar]] is a list of parameters that will be needed by
logfun.
In R the ones that are simple copies such as [[ofile]] would not need to
be included in the list since they are inherited with the environment,
however, I prefer to make such hidden arguments explicit.
<<coxme-fit>>=
<<define-logfun>>
if (length(theta)) {
    logpar <- list(varlist=varlist, parmlist=parmlist,
                   kfun=kfun, init=c(rep(0., npenal), fit0$coef),
                   fit0= fit0$loglik[2],
                   iter=coxme.control$inner.iter,
                   ofile=ofile)
  
    mfit <- do.call('optim', c(list(par= theta[!fixed], fn=logfun, gr=NULL), 
                           optpar, logpar))
    theta <- mfit$par
    }
@
The optimization finds the best value of theta, but does not return
all the parameters we need from the fit.  So we make one more call.
This is essentially the ``inside'' of [[logfun]].
<<coxme-fit>>=
    gkmat <- gchol(kfun(theta, varlist, parmlist))
    ikmat <- solve(gkmat)  #inverse of kmat, which is the penalty
    fit <- .C(ofile,
              iter= as.integer(c(0, control$iter.max)),
              beta = as.double(c(rep(0., nfrail), fit0$coef)),
              loglik = double(2),
              as.double(ikmat@blocks),
              as.double(ikmat@rmat),
              hdet = double(1))
    ilik <- fit$loglik[2] -
             .5*(sum(log(diag(gkmat))) + fit$hdet)
 <<refine>>
@
              
\subsection{Finishing up}
\label{sect:final}
There are 4 tasks left to do
<<coxme-finish>>=
 <<release-memory>>
 <<coxme-rescale>>
 <<coxme-df>>
 <<create-output-list>>
@ 

This routine finishes up with the C code.
The first few lines reprise some variables
found in the C code but not before needed here.
It returns the score vector $u$, the 
sparse and dense portions of the cholesky decompostion of the
hessian matrix (h.b and h.r), the
inverse hessian matrix (hi.b, hi.r), and the rank of the final 
solution.  These are needed to compute the variance matrix of the
estimates.
It then releases all the memory allocated by the initial routine.
<<release-memory>>=
nfrail <- nrow(ikmat)  #total number of penalized terms
nvar2  <- nvar + (nfrail - nsparse)  # total number of non-sparse coefs
nvar3  <- nvar + nfrail              # total number of coefficients
btot   <- length(ikmat@blocks)

fit3 <- .C('coxfit6c',
               u    = double(nvar3),
               h.b  = double(btot),
               h.r  = double(nvar2*nvar3),
               hi.b = double(btot),
               hi.r = double(nvar2*nvar3),
               hrank= integer(1),
               as.integer(ncol(y)),
               )
@ 

Now create the hessian and inverse hessian matrices; the latter of
these is the variance matrix.  
The C code had centered and rescaled all $X$ matrix coefficients
so we need to undo that scaling. 
First we deal with a special case, if there are only sparse terms
then [[hmat]] and [[hinv]] have only a block-diagonal component.
(This happens more often than you might think, a random per-subject
intercept for instance.)
<<coxme-rescale>>=
    if (nvar2 ==0) {
        hmat <- new('gchol.bdsmatrix', .Dim=c(nvar3, nvar3),
                    blocksize=ikmat@blocksize, blocks=fit3$h.b,
                    rmat=numeric(0), rank=fit3$hrank)
        hinv <- bdsmatrix(blocksize=ikmat@blocksize, blocks=fit3$hi.b)
        }
@ 
And now three cases: no $X$ variables, a single $X$, or multiple $X$
variables.
Assume there are $p$=nvar variables and let [[V]] be the lower $p \times p$
portion of the [[nvar3]] by [[nvar2]] $R$ matrix,
and $S$ = [[diag(scale]]
be the rescaling vector.
$X$ was replaced by $X S^{-1}$ before computation.
For the hessian, we want to replace $V$ with $SVS$ and for the
inverse hessian with $S^{-1}V S^{-1}$.
The matrix [[hmat]] is however a cholesky decomposition of the
hessian $H=LDL'$ where $L$ is lower triangular with ones on the
diagonal and $D$ is diagonal; $D$ is kept on the diagonal of $V$ and
$L$ below the diagonal.
A little algebra shows that we want to replace $D$ (the diagonal of $L$)
with $S^2D$ and $L$ with SLS^{-1}.
<<coxme-rescale>>=
    else {
        rmat1 <- matrix(fit3$h.r, nrow=nvar3)
        rmat2 <- matrix(fit3$hi.r, nrow=nvar3)
        if (nvar ==1 ) {
            rmat1[nvar3,] <- rmat1[nvar3,]/scale
            rmat2[nvar3,] <- rmat2[nvar3,]/scale
            rmat1[,nvar2] <- rmat1[,nvar2]*scale
            rmat2[,nvar2] <- rmat2[,nvar2]/scale
            rmat1[nvar3,nvar2] <- rmat1[nvar3,nvar2]*scale^2
            u <- fit3$u  # the efficient score vector U
            u[nvar3] <- u[nvar3]*scale
            }
        else if (nvar >1) {
            temp <- (nvar3-nvar):nvar3 
            u <- fit$u
            u[temp] <- u[temp]*scale
            rmat1[temp,] <- (1/scale)*rmat1[temp,] #multiply rows* scale
            rmat2[temp,] <- (1/scale)*rmat2[temp,] 

            temp <- (nvar2-nvar):nvar2        #multiply cols
            rmat1[,temp] <- rmat1[,temp] %*% diag(scale)
            rmat2[,temp] <- rmat2[,temp] %*% diag(1/scale)
            temp <- seq(length=length(scale), to=length(rmat1), by=1+nvar3)
            rmat1[temp] <- rmat1[temp]*(scale^2)    #fix the diagonal
            }
        hmat <- new('gchol.bdsmatrix', .Dim=c(nvar3, nvar3),
                    blocksize=ikmat@blocksize, blocks=fit3$h.b,
                    rmat= rmat1, rank=fit3$hrank)
        hinv <- bdsmatrix(blocksize=ikmat@blocksize, blocks=fit3$hi.b,
                          rmat=rmat2)
        }
@             

Now for the degrees of freedom, which is formula 5.16 of Therneau & Grambsch
First we have a small utility function to compute the ${\rm trace}(AB)$ where
$A$ and $B$ are bdsmatrix objects.  
For ordinary matrices this is the sum of the elementwise product of $A$ and
$B'$, but we have to account for the fact that bdsmatrix objects only
keep the lower diagonal, so we need the diagonal sum + 2 times the off-diagonal
sum.  
<<coxme-df>>=
traceprod <- function(H, P) {
    #block-diagonal portions match
    nfrail <- nrow(P)  #penalty matrix
    temp1 <- sum(H@blocks * P@blocks)
    if (length(P@rmat) >0) {
        #I only want the penalized part of H
        rd <- dim(P@rmat)
        temp1 <- temp1 + sum(H@rmat[1:rd[1], 1:rd[2]] * P@rmat)
        }
    2*temp1 - sum(diag(H)[1:nfrail] * diag(P))
    }
df <- nvar + (nfrail - traceprod(hinv, ikmat))
@ 

And last, put together the output structure.
<<create-output-list>>=
    penalty <- sum(fcoef * (ikmat %*% fcoef))
    idf <- nvar + sum(ntheta)

    if (nvar > 0) {
	out <- list(coefficients=list(fixed=fit$beta[-(1:nfrail)]/scale, 
	                       random=theta),
	     frail=fit$beta[1:nfrail], penalty=penalty,
	     loglik=c(fit0$log[1], ilik, fit$log[2]), var=hinv,
	     df=c(idf, df), hmat=hmat, iter=iter, control=control,
	     u=u, means=means, scale=scale)
	}
    else out <- list(coefficients=list(fixed=NULL, random=theta),
	      frail=fit$beta[1:nfrail], penalty= penalty,
	      loglik=c(fit0$log[1], ilik, fit$log[2]), var=hinv,
	      df=c(idf, df), hmat=hmat, iter=iter, control=control,
	      u=fit3$u, means=means, scale=scale)    

    if (refine.n>0) out<- c(out, list(errhat=errhat, refine=r.correct))

    out
    }
@ 
\end{document}
